{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:13:52.082600Z","iopub.execute_input":"2022-03-27T18:13:52.083189Z","iopub.status.idle":"2022-03-27T18:13:59.538732Z","shell.execute_reply.started":"2022-03-27T18:13:52.083146Z","shell.execute_reply":"2022-03-27T18:13:59.537791Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.16.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:13:59.541053Z","iopub.execute_input":"2022-03-27T18:13:59.541333Z","iopub.status.idle":"2022-03-27T18:14:07.301781Z","shell.execute_reply.started":"2022-03-27T18:13:59.541298Z","shell.execute_reply":"2022-03-27T18:14:07.300907Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.18.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.3)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.4.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.20.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.26.0)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.9)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom transformers import BertModel, AutoTokenizer, BertTokenizer, PreTrainedTokenizerFast, AdamW, get_linear_schedule_with_warmup\nfrom transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DistilBertTokenizerFast\nimport torch.nn.functional as F\nfrom datasets import load_dataset\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom collections import defaultdict\nfrom textwrap import wrap\nfrom torch import nn, optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import CrossEntropyLoss\n\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:07.304497Z","iopub.execute_input":"2022-03-27T18:14:07.304777Z","iopub.status.idle":"2022-03-27T18:14:07.313781Z","shell.execute_reply.started":"2022-03-27T18:14:07.304736Z","shell.execute_reply":"2022-03-27T18:14:07.312918Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:07.316200Z","iopub.execute_input":"2022-03-27T18:14:07.317534Z","iopub.status.idle":"2022-03-27T18:14:07.331147Z","shell.execute_reply.started":"2022-03-27T18:14:07.317496Z","shell.execute_reply":"2022-03-27T18:14:07.330358Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:07.334283Z","iopub.execute_input":"2022-03-27T18:14:07.334564Z","iopub.status.idle":"2022-03-27T18:14:07.340674Z","shell.execute_reply.started":"2022-03-27T18:14:07.334538Z","shell.execute_reply":"2022-03-27T18:14:07.339909Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"imdb_dataset = load_dataset('imdb')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:07.342089Z","iopub.execute_input":"2022-03-27T18:14:07.342414Z","iopub.status.idle":"2022-03-27T18:14:07.882969Z","shell.execute_reply.started":"2022-03-27T18:14:07.342366Z","shell.execute_reply":"2022-03-27T18:14:07.882363Z"},"trusted":true},"execution_count":127,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1edbc6547a9242fb8df00c76d8b787a2"}},"metadata":{}}]},{"cell_type":"code","source":"imdb_dataset = imdb_dataset.shuffle(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:07.886667Z","iopub.execute_input":"2022-03-27T18:14:07.888803Z","iopub.status.idle":"2022-03-27T18:14:07.903579Z","shell.execute_reply.started":"2022-03-27T18:14:07.888760Z","shell.execute_reply":"2022-03-27T18:14:07.902750Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"train_texts = imdb_dataset[\"train\"][\"text\"][:4000]\ntrain_labels = imdb_dataset[\"train\"][\"label\"][:4000]\ntest_texts = imdb_dataset[\"test\"][\"text\"][:1000]\ntest_labels = imdb_dataset[\"test\"][\"label\"][:1000]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:07.905245Z","iopub.execute_input":"2022-03-27T18:14:07.905801Z","iopub.status.idle":"2022-03-27T18:14:08.587503Z","shell.execute_reply.started":"2022-03-27T18:14:07.905766Z","shell.execute_reply":"2022-03-27T18:14:08.586694Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"labels_num = len(set(test_labels))\nlabels_num","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:08.589205Z","iopub.execute_input":"2022-03-27T18:14:08.589473Z","iopub.status.idle":"2022-03-27T18:14:08.594945Z","shell.execute_reply.started":"2022-03-27T18:14:08.589436Z","shell.execute_reply":"2022-03-27T18:14:08.594218Z"},"trusted":true},"execution_count":130,"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"del imdb_dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:08.598423Z","iopub.execute_input":"2022-03-27T18:14:08.598936Z","iopub.status.idle":"2022-03-27T18:14:08.606510Z","shell.execute_reply.started":"2022-03-27T18:14:08.598898Z","shell.execute_reply":"2022-03-27T18:14:08.605577Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:08.607988Z","iopub.execute_input":"2022-03-27T18:14:08.608244Z","iopub.status.idle":"2022-03-27T18:14:08.618953Z","shell.execute_reply.started":"2022-03-27T18:14:08.608210Z","shell.execute_reply":"2022-03-27T18:14:08.618020Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:08.620811Z","iopub.execute_input":"2022-03-27T18:14:08.621090Z","iopub.status.idle":"2022-03-27T18:14:10.075685Z","shell.execute_reply.started":"2022-03-27T18:14:08.621053Z","shell.execute_reply":"2022-03-27T18:14:10.074974Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stderr","text":"https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsvpmg9hx\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6bf8cbef1f4f02a05c296d2d3fa002"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\ncreating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nhttps://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxp__1ecs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1cbb325429f41bca7b29c2dfc4b4a5a"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\ncreating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nhttps://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppewfh_jr\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34068c25769b4b789efefebe3d9dec3a"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\ncreating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\nloading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\nloading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nhttps://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnaz0fwdu\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0905744b6b2c485390affce4861cef44"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\ncreating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nloading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BertTokenizer'. \nThe class this function is called from is 'DistilBertTokenizerFast'.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)\nval_encodings = tokenizer(val_texts,truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:10.077379Z","iopub.execute_input":"2022-03-27T18:14:10.077654Z","iopub.status.idle":"2022-03-27T18:14:13.998165Z","shell.execute_reply.started":"2022-03-27T18:14:10.077618Z","shell.execute_reply":"2022-03-27T18:14:13.997441Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"class IMDbDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = IMDbDataset(train_encodings, train_labels)\nval_dataset = IMDbDataset(val_encodings, val_labels)\ntest_dataset = IMDbDataset(test_encodings, test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:13.999451Z","iopub.execute_input":"2022-03-27T18:14:13.999706Z","iopub.status.idle":"2022-03-27T18:14:14.040305Z","shell.execute_reply.started":"2022-03-27T18:14:13.999673Z","shell.execute_reply":"2022-03-27T18:14:14.039586Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:14.041612Z","iopub.execute_input":"2022-03-27T18:14:14.042193Z","iopub.status.idle":"2022-03-27T18:14:14.050649Z","shell.execute_reply.started":"2022-03-27T18:14:14.042155Z","shell.execute_reply":"2022-03-27T18:14:14.049846Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:14.051649Z","iopub.execute_input":"2022-03-27T18:14:14.053529Z","iopub.status.idle":"2022-03-27T18:14:14.059980Z","shell.execute_reply.started":"2022-03-27T18:14:14.053491Z","shell.execute_reply":"2022-03-27T18:14:14.059227Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=8,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n    report_to=None,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:14.061432Z","iopub.execute_input":"2022-03-27T18:14:14.061941Z","iopub.status.idle":"2022-03-27T18:14:14.080343Z","shell.execute_reply.started":"2022-03-27T18:14:14.061904Z","shell.execute_reply":"2022-03-27T18:14:14.079704Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:14.081294Z","iopub.execute_input":"2022-03-27T18:14:14.081940Z","iopub.status.idle":"2022-03-27T18:14:24.895148Z","shell.execute_reply.started":"2022-03-27T18:14:14.081905Z","shell.execute_reply":"2022-03-27T18:14:24.894444Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nhttps://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpg022675m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b6d824dedb4ccbb530f3a9007e2d23"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\ncreating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# BertForSequenceClassification source code\n\nclass SentimentClassifier(nn.Module):\n\n    def __init__(self, n_classes):\n\n    super().__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(p=0.3)\n    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    self.n_classes = n_classes\n\n    def forward(self,        \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None\n    ):\n\n    last_hidden_state, pooled_output = self.bert(input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        return_dict=False)\n\n    output = self.drop(pooled_output)\n    logits = self.out(output)\n\n    loss = None\n    if labels is not None:\n        loss_fct = CrossEntropyLoss()\n        loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n    output = (logits,)\n    return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:24.896755Z","iopub.execute_input":"2022-03-27T18:14:24.897036Z","iopub.status.idle":"2022-03-27T18:14:24.905862Z","shell.execute_reply.started":"2022-03-27T18:14:24.896999Z","shell.execute_reply":"2022-03-27T18:14:24.905025Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifier(labels_num)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:24.906936Z","iopub.execute_input":"2022-03-27T18:14:24.907293Z","iopub.status.idle":"2022-03-27T18:14:26.552862Z","shell.execute_reply.started":"2022-03-27T18:14:24.907250Z","shell.execute_reply":"2022-03-27T18:14:26.552135Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:26.554255Z","iopub.execute_input":"2022-03-27T18:14:26.554543Z","iopub.status.idle":"2022-03-27T18:14:26.563481Z","shell.execute_reply.started":"2022-03-27T18:14:26.554496Z","shell.execute_reply":"2022-03-27T18:14:26.562675Z"},"trusted":true},"execution_count":142,"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"SentimentClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.3, inplace=False)\n  (out): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"trainer = Trainer(\n          model=model,\n          args=training_args,\n          train_dataset=train_dataset,\n          eval_dataset=val_dataset,\n          compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:26.564876Z","iopub.execute_input":"2022-03-27T18:14:26.566016Z","iopub.status.idle":"2022-03-27T18:14:26.580426Z","shell.execute_reply.started":"2022-03-27T18:14:26.565976Z","shell.execute_reply":"2022-03-27T18:14:26.579762Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:14:26.581976Z","iopub.execute_input":"2022-03-27T18:14:26.582445Z","iopub.status.idle":"2022-03-27T18:20:38.255805Z","shell.execute_reply.started":"2022-03-27T18:14:26.582407Z","shell.execute_reply":"2022-03-27T18:20:38.254751Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 3200\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 800\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [800/800 06:11, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.706700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.694900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.736500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.696000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.709100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.703600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.687500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.687800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.655500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.681100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.648100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.657400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.605800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.496800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.539100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.569500</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.351700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.335800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.454700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.298200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.428700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.190400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.208800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.488200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.430800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.380100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.294800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.418100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.452200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.292200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.441800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.394300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.176000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.863700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.366700</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.304600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.339000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.436900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.280400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.294100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.361800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.282700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.210200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.210700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.222100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.307200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.414900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.340200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.218100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.253800</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.512400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.484800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.467400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.207600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.244100</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.294300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.714100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.333600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.199400</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.320600</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.240200</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.169600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.255800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.287600</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.309300</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.463900</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.179900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.420600</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.202200</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.204100</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.181900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.174000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.176800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.409500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.295300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.318800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.546600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=800, training_loss=0.3994746758043766, metrics={'train_runtime': 371.6268, 'train_samples_per_second': 17.222, 'train_steps_per_second': 2.153, 'total_flos': 0.0, 'train_loss': 0.3994746758043766, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:38.257275Z","iopub.execute_input":"2022-03-27T18:20:38.257770Z","iopub.status.idle":"2022-03-27T18:20:52.815382Z","shell.execute_reply.started":"2022-03-27T18:20:38.257734Z","shell.execute_reply":"2022-03-27T18:20:52.814741Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 800\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='113' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:32]\n    </div>\n    "},"metadata":{}},{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.3862309157848358,\n 'eval_accuracy': 0.88875,\n 'eval_f1': 0.8808567603748326,\n 'eval_precision': 0.8844086021505376,\n 'eval_recall': 0.8773333333333333,\n 'eval_runtime': 14.5483,\n 'eval_samples_per_second': 54.989,\n 'eval_steps_per_second': 3.437,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:52.816476Z","iopub.execute_input":"2022-03-27T18:20:52.816721Z","iopub.status.idle":"2022-03-27T18:21:11.039958Z","shell.execute_reply.started":"2022-03-27T18:20:52.816688Z","shell.execute_reply":"2022-03-27T18:21:11.039021Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3936313986778259,\n 'test_accuracy': 0.891,\n 'test_f1': 0.8893401015228426,\n 'test_precision': 0.8812877263581489,\n 'test_recall': 0.8975409836065574,\n 'test_runtime': 18.2153,\n 'test_samples_per_second': 54.899,\n 'test_steps_per_second': 3.459,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# CLS","metadata":{}},{"cell_type":"code","source":"class SentimentClassifierCLS(nn.Module):\n\n    def __init__(self, n_classes):\n\n    super().__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(p=0.3)\n    self.linear = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n    self.out = nn.Linear(self.bert.config.hidden_size + 768, n_classes)\n    self.n_classes = n_classes\n\n    def forward(self,        \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None\n    ):\n\n    last_hidden_state, pooled_output = self.bert(input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        return_dict=False)\n\n    cls = last_hidden_state[:,0,:]\n    pooled_output = self.linear(self.drop(pooled_output)) \n    stacked_layers = torch.hstack([cls, pooled_output])\n    logits = self.out(stacked_layers)\n\n    loss = None\n    if labels is not None:\n        loss_fct = CrossEntropyLoss()\n        loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n    output = (logits,)\n    return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:11.041325Z","iopub.execute_input":"2022-03-27T18:21:11.041599Z","iopub.status.idle":"2022-03-27T18:21:11.051491Z","shell.execute_reply.started":"2022-03-27T18:21:11.041564Z","shell.execute_reply":"2022-03-27T18:21:11.050813Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifierCLS(labels_num)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:11.052688Z","iopub.execute_input":"2022-03-27T18:21:11.053068Z","iopub.status.idle":"2022-03-27T18:21:12.488445Z","shell.execute_reply.started":"2022-03-27T18:21:11.053031Z","shell.execute_reply":"2022-03-27T18:21:12.487100Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n          model=model,\n          args=training_args,\n          train_dataset=train_dataset,\n          eval_dataset=val_dataset,\n          compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:12.495196Z","iopub.execute_input":"2022-03-27T18:21:12.499089Z","iopub.status.idle":"2022-03-27T18:21:12.518061Z","shell.execute_reply.started":"2022-03-27T18:21:12.499051Z","shell.execute_reply":"2022-03-27T18:21:12.517439Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:12.521413Z","iopub.execute_input":"2022-03-27T18:21:12.523198Z","iopub.status.idle":"2022-03-27T18:27:24.314970Z","shell.execute_reply.started":"2022-03-27T18:21:12.523164Z","shell.execute_reply":"2022-03-27T18:27:24.314235Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 3200\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 800\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [800/800 06:11, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.706300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.677100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.668500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.684200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.678200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.673600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.634600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.647000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.577800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.450900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.457800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.359700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.370100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.257700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.513800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.516000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.462300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.408800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.649900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.321900</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.470400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.143300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.174800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.516700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.455400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.376300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.225800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.314600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.481400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.263200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.733000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.421100</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.229900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.243200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.500500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.220200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.237000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.439100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.398600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.295000</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.320000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.353000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.171700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.241600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.220300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.329100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.250700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.230500</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.343500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.385500</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.223800</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.188200</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.233800</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.305000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.390300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.209100</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.166900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.070600</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.777400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.497900</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.132800</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.308600</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.263000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.109900</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.321000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.100700</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.427500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.329000</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.228300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.267100</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.177800</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.213700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.057900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.417800</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.184800</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.284400</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.172300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.427800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":150,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=800, training_loss=0.35496791936457156, metrics={'train_runtime': 371.767, 'train_samples_per_second': 17.215, 'train_steps_per_second': 2.152, 'total_flos': 0.0, 'train_loss': 0.35496791936457156, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:27:24.317199Z","iopub.execute_input":"2022-03-27T18:27:24.317637Z","iopub.status.idle":"2022-03-27T18:27:38.887580Z","shell.execute_reply.started":"2022-03-27T18:27:24.317605Z","shell.execute_reply":"2022-03-27T18:27:38.886868Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 800\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='113' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:32]\n    </div>\n    "},"metadata":{}},{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.3989125192165375,\n 'eval_accuracy': 0.90625,\n 'eval_f1': 0.9009247027741083,\n 'eval_precision': 0.8926701570680629,\n 'eval_recall': 0.9093333333333333,\n 'eval_runtime': 14.5601,\n 'eval_samples_per_second': 54.945,\n 'eval_steps_per_second': 3.434,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:27:38.888944Z","iopub.execute_input":"2022-03-27T18:27:38.889194Z","iopub.status.idle":"2022-03-27T18:27:57.099915Z","shell.execute_reply.started":"2022-03-27T18:27:38.889159Z","shell.execute_reply":"2022-03-27T18:27:57.099269Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.39849212765693665,\n 'test_accuracy': 0.9,\n 'test_f1': 0.8995983935742973,\n 'test_precision': 0.8818897637795275,\n 'test_recall': 0.9180327868852459,\n 'test_runtime': 18.2028,\n 'test_samples_per_second': 54.937,\n 'test_steps_per_second': 3.461,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model to data","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:44.050094Z","iopub.execute_input":"2022-03-27T18:34:44.050477Z","iopub.status.idle":"2022-03-27T18:34:44.054987Z","shell.execute_reply.started":"2022-03-27T18:34:44.050319Z","shell.execute_reply":"2022-03-27T18:34:44.054289Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:44.056261Z","iopub.execute_input":"2022-03-27T18:34:44.056751Z","iopub.status.idle":"2022-03-27T18:34:45.500999Z","shell.execute_reply.started":"2022-03-27T18:34:44.056715Z","shell.execute_reply":"2022-03-27T18:34:45.500287Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n          model=model,\n          args=training_args,\n          train_dataset=train_dataset,\n          eval_dataset=val_dataset,\n          compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:45.502167Z","iopub.execute_input":"2022-03-27T18:34:45.502458Z","iopub.status.idle":"2022-03-27T18:34:45.515192Z","shell.execute_reply.started":"2022-03-27T18:34:45.502419Z","shell.execute_reply":"2022-03-27T18:34:45.514524Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:45.516975Z","iopub.execute_input":"2022-03-27T18:34:45.517238Z","iopub.status.idle":"2022-03-27T18:40:58.069037Z","shell.execute_reply.started":"2022-03-27T18:34:45.517202Z","shell.execute_reply":"2022-03-27T18:40:58.068277Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 3200\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 800\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [800/800 06:12, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.712600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.694900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.677300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.679700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.678800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.651700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.676000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.575900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.522400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.546900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.474300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.417100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.300900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.487800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.491900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.582300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.590400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.453900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.303000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.511800</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.283600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.183200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.323700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.466800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.454400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.271100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.352700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.392800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.231900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.597500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.377600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.241100</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.224900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.449800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.321800</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.251800</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.351800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.473200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.357300</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.407900</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.283500</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.238600</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.259900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.229400</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.374500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.290100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.547700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.525500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.273200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.219100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.297300</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.244900</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.273700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.437900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.447600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.181400</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.355200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.286500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.308000</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.301400</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.414700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.193200</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.089700</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.296300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.194300</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.495700</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.415600</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.210600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.385400</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.162600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.142600</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.184600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.045500</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.255400</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.367100</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.306900</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.326600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.191600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.407600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":163,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=800, training_loss=0.37710245288908484, metrics={'train_runtime': 372.5274, 'train_samples_per_second': 17.18, 'train_steps_per_second': 2.147, 'total_flos': 1683910754304000.0, 'train_loss': 0.37710245288908484, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:40:58.070225Z","iopub.execute_input":"2022-03-27T18:40:58.070556Z","iopub.status.idle":"2022-03-27T18:41:12.631551Z","shell.execute_reply.started":"2022-03-27T18:40:58.070517Z","shell.execute_reply":"2022-03-27T18:41:12.630826Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 800\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='113' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:04]\n    </div>\n    "},"metadata":{}},{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.3261491358280182,\n 'eval_accuracy': 0.89875,\n 'eval_f1': 0.8921438082556591,\n 'eval_precision': 0.8909574468085106,\n 'eval_recall': 0.8933333333333333,\n 'eval_runtime': 14.5525,\n 'eval_samples_per_second': 54.973,\n 'eval_steps_per_second': 3.436,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:41:44.942106Z","iopub.execute_input":"2022-03-27T18:41:44.942408Z","iopub.status.idle":"2022-03-27T18:42:03.184116Z","shell.execute_reply.started":"2022-03-27T18:41:44.942363Z","shell.execute_reply":"2022-03-27T18:42:03.183320Z"},"trusted":true},"execution_count":165,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"execution_count":165,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3445480465888977,\n 'test_accuracy': 0.899,\n 'test_f1': 0.8999008919722497,\n 'test_precision': 0.8714011516314779,\n 'test_recall': 0.930327868852459,\n 'test_runtime': 18.2345,\n 'test_samples_per_second': 54.841,\n 'test_steps_per_second': 3.455,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Google Play reviews","metadata":{}},{"cell_type":"markdown","source":"## 1/5 stars","metadata":{}},{"cell_type":"code","source":"review = \"Shows how greed and drugs turn people ugly. Not one character in this movie that makes you feel good. Didn't laugh once, didn't feel good once, didn't learn anything = waste of time (and since it's a long movie, lot's of time wasted). I watched it until the end hoping for something uplifting at the end but it didn't come. Good acting but lot's of foul language. This movie also failed to show where does all the money on Wall Street come from. It missed the opportunity to show how amateur investors lose their hard earned money and the consequences because of the promise of big returns by the Wolf's greedy gang.\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:44:47.625811Z","iopub.execute_input":"2022-03-27T18:44:47.626085Z","iopub.status.idle":"2022-03-27T18:44:47.629872Z","shell.execute_reply.started":"2022-03-27T18:44:47.626057Z","shell.execute_reply":"2022-03-27T18:44:47.629193Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:44:48.252105Z","iopub.execute_input":"2022-03-27T18:44:48.252550Z","iopub.status.idle":"2022-03-27T18:44:48.261668Z","shell.execute_reply.started":"2022-03-27T18:44:48.252512Z","shell.execute_reply":"2022-03-27T18:44:48.260907Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"model(\n  input_ids=encoding['input_ids'], \n  attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:44:49.196161Z","iopub.execute_input":"2022-03-27T18:44:49.196784Z","iopub.status.idle":"2022-03-27T18:44:49.228479Z","shell.execute_reply.started":"2022-03-27T18:44:49.196743Z","shell.execute_reply":"2022-03-27T18:44:49.227716Z"},"trusted":true},"execution_count":169,"outputs":[{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"   1/1,  ,       .","metadata":{}},{"cell_type":"markdown","source":"## 3/5 stars","metadata":{}},{"cell_type":"code","source":"review = \"It's one of those movies that has you waiting for that moment that never comes. Although it was based on true events, it feels like the film creators didn't offer that 'BOOM!' That brings you and the film to a halt and back to reality. It felt like the movie was advocating the behavior that this lifestyle led M.Belfort to live. Leonardo's acting is extravagant, ruthless and insane at times, which saves the movie's balls at times. Oh, and Jonah Hill was good, too, I suppose. All in all, overrated but OK.\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:45:06.443162Z","iopub.execute_input":"2022-03-27T18:45:06.443443Z","iopub.status.idle":"2022-03-27T18:45:06.447532Z","shell.execute_reply.started":"2022-03-27T18:45:06.443389Z","shell.execute_reply":"2022-03-27T18:45:06.446769Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:45:06.958955Z","iopub.execute_input":"2022-03-27T18:45:06.959210Z","iopub.status.idle":"2022-03-27T18:45:06.965615Z","shell.execute_reply.started":"2022-03-27T18:45:06.959182Z","shell.execute_reply":"2022-03-27T18:45:06.964758Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"model(\n  input_ids=encoding['input_ids'], \n  attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:45:07.577157Z","iopub.execute_input":"2022-03-27T18:45:07.577690Z","iopub.status.idle":"2022-03-27T18:45:07.611001Z","shell.execute_reply.started":"2022-03-27T18:45:07.577644Z","shell.execute_reply":"2022-03-27T18:45:07.610340Z"},"trusted":true},"execution_count":172,"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"   3/5, .   ,     .     0 ().","metadata":{}},{"cell_type":"markdown","source":"## 5/5 stars","metadata":{}},{"cell_type":"code","source":"review = \"My New Favorite Movie! This movie grabbed my eyes and ears less than 2 mins into it (which is very rare for me), and did not let go until the end. It made me feel everything: Happiness, sadness, anger, jealousy, pity etc. It is a movie about greed, power and pride, and is packed with sex, money, and drugs. And I mean PACKED. So not everyone may enjoy it. But it is a true story about a financially elite man, and his privileged lifestyle. Excellent Movie All Around!\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:45:09.603196Z","iopub.execute_input":"2022-03-27T18:45:09.603922Z","iopub.status.idle":"2022-03-27T18:45:09.610087Z","shell.execute_reply.started":"2022-03-27T18:45:09.603885Z","shell.execute_reply":"2022-03-27T18:45:09.609311Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:45:10.182564Z","iopub.execute_input":"2022-03-27T18:45:10.183090Z","iopub.status.idle":"2022-03-27T18:45:10.189355Z","shell.execute_reply.started":"2022-03-27T18:45:10.183052Z","shell.execute_reply":"2022-03-27T18:45:10.188465Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"model(\n  input_ids=encoding['input_ids'], \n  attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:45:10.770284Z","iopub.execute_input":"2022-03-27T18:45:10.770530Z","iopub.status.idle":"2022-03-27T18:45:10.802289Z","shell.execute_reply.started":"2022-03-27T18:45:10.770503Z","shell.execute_reply":"2022-03-27T18:45:10.801654Z"},"trusted":true},"execution_count":175,"outputs":[{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"markdown","source":"   ,   5/5 ,        .","metadata":{}}]}