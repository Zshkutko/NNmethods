{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "67F69YijD3d6",
        "execution": {
          "iopub.status.busy": "2021-11-26T18:49:08.319643Z",
          "iopub.execute_input": "2021-11-26T18:49:08.320095Z",
          "iopub.status.idle": "2021-11-26T18:51:38.157783Z",
          "shell.execute_reply.started": "2021-11-26T18:49:08.320011Z",
          "shell.execute_reply": "2021-11-26T18:51:38.156969Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a294bc4e-c8d1-44ea-e554-a6bd66bba8b6"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.7/dist-packages (0.13.9)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: ipython>=7.17.0 in /usr/local/lib/python3.7/dist-packages (from ipdb) (7.30.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (3.0.23)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sRNAtqAKSWA",
        "execution": {
          "iopub.status.busy": "2021-11-26T18:51:38.159970Z",
          "iopub.execute_input": "2021-11-26T18:51:38.160247Z",
          "iopub.status.idle": "2021-11-26T18:52:07.251665Z",
          "shell.execute_reply.started": "2021-11-26T18:51:38.160215Z",
          "shell.execute_reply": "2021-11-26T18:52:07.250894Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75862601-def9-404f-92d2-0f125701402d"
      },
      "source": [
        "!pip install torchmetrics"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbm9Sn1gD57T"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "import ipdb\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from gensim.models import FastText\n",
        "from torchmetrics.functional import f1, recall"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxerF2AvcbhQ"
      },
      "source": [
        "#Dataset1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dm_3VK-D8Yx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bc07894b-3468-4e67-df32-925dba881a9d"
      },
      "source": [
        "'''!wget -O positive.csv https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0\n",
        "!wget -O negative.csv https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0'''"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!wget -O positive.csv https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0\\n!wget -O negative.csv https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcm-qrcsD_M2"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7mRRiHfEB-v"
      },
      "source": [
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9hhOev_EELi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29aeeff5-01a9-46d7-ddb2-605de5bead4e"
      },
      "source": [
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "print(len(all_tweets_data))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjge8NdqELkf"
      },
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIsO1Fy6ELtQ"
      },
      "source": [
        "def preprocess(text):\n",
        "      text = text.lower().replace(\"ё\", \"е\")\n",
        "      text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
        "      text = re.sub('@[^\\s]+', 'USER', text)\n",
        "      text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "      text = re.sub(' +', ' ', text)\n",
        "      return text.strip()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4npW0EoFN-NA"
      },
      "source": [
        "clean_text = tweets_data['text'].apply(preprocess)\n",
        "tweets_data['clean_text'] = clean_text"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMRUg3gVZm7M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "f47ce71f-393f-4a54-840e-78aa7b691e2e"
      },
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)\n",
        "train_sentences"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tone</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>99397</th>\n",
              "      <td>OPEN PACK 1 000 000 COINS!!! R172184516292 - с...</td>\n",
              "      <td>0</td>\n",
              "      <td>open pack 1 coins r172184516292 сбор бомжу на ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89784</th>\n",
              "      <td>Вытощили меня из дому, так не хотела идти((</td>\n",
              "      <td>0</td>\n",
              "      <td>вытощили меня из дому так не хотела идти</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109488</th>\n",
              "      <td>RT @BureevaMarina: - Хотела отложить денег на ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rt USER хотела отложить денег на старость не п...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98153</th>\n",
              "      <td>RT @lentaruofficial: План выступления такой: 1...</td>\n",
              "      <td>1</td>\n",
              "      <td>rt USER план выступления такой 1 социалочка на...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37953</th>\n",
              "      <td>@5tv_news А зачем Вы прерывались на рекламу то...</td>\n",
              "      <td>0</td>\n",
              "      <td>USER а зачем вы прерывались на рекламу то он п...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19958</th>\n",
              "      <td>@Chynara_B еще как! Ну что, чувствуешь себя ме...</td>\n",
              "      <td>1</td>\n",
              "      <td>USER еще как ну что чувствуешь себя менее эгои...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89455</th>\n",
              "      <td>обажаю Арию) и я не о музыкантах, а о девочке ...</td>\n",
              "      <td>1</td>\n",
              "      <td>обажаю арию и я не о музыкантах а о девочке из...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20770</th>\n",
              "      <td>какая \"классная\" эта пятница 13-ое(( голова уж...</td>\n",
              "      <td>0</td>\n",
              "      <td>какая классная эта пятница 13 ое голова уже по...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3382</th>\n",
              "      <td>@KovalenkoIgor В Украине ее видать внимательно...</td>\n",
              "      <td>1</td>\n",
              "      <td>USER в украине ее видать внимательно читали</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107630</th>\n",
              "      <td>RT @BigRussianBaws: Не понимаю людей, которые ...</td>\n",
              "      <td>0</td>\n",
              "      <td>rt USER не понимаю людей которые обтягивают зо...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  ...                                         clean_text\n",
              "99397   OPEN PACK 1 000 000 COINS!!! R172184516292 - с...  ...  open pack 1 coins r172184516292 сбор бомжу на ...\n",
              "89784         Вытощили меня из дому, так не хотела идти((  ...           вытощили меня из дому так не хотела идти\n",
              "109488  RT @BureevaMarina: - Хотела отложить денег на ...  ...  rt USER хотела отложить денег на старость не п...\n",
              "98153   RT @lentaruofficial: План выступления такой: 1...  ...  rt USER план выступления такой 1 социалочка на...\n",
              "37953   @5tv_news А зачем Вы прерывались на рекламу то...  ...  USER а зачем вы прерывались на рекламу то он п...\n",
              "...                                                   ...  ...                                                ...\n",
              "19958   @Chynara_B еще как! Ну что, чувствуешь себя ме...  ...  USER еще как ну что чувствуешь себя менее эгои...\n",
              "89455   обажаю Арию) и я не о музыкантах, а о девочке ...  ...  обажаю арию и я не о музыкантах а о девочке из...\n",
              "20770   какая \"классная\" эта пятница 13-ое(( голова уж...  ...  какая классная эта пятница 13 ое голова уже по...\n",
              "3382    @KovalenkoIgor В Украине ее видать внимательно...  ...        USER в украине ее видать внимательно читали\n",
              "107630  RT @BigRussianBaws: Не понимаю людей, которые ...  ...  rt USER не понимаю людей которые обтягивают зо...\n",
              "\n",
              "[90000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hxzI58XELve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56031afc-c3b9-442f-86f7-abf055b10c88"
      },
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in tweets_data['clean_text']:\n",
        "    vocab.update(preprocess(text).split())\n",
        "print('всего уникальных токенов:', len(vocab))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 111486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6hFgChiELy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185af0a5-c2fc-471c-eca8-f41c7d2f4825"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, втретившихся больше 2 раз: 29074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQflLq0YEet3"
      },
      "source": [
        "#создаем словарь с индексами word2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-hI2uD0EewU"
      },
      "source": [
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQGexPjtEezn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72501d0-e266-4eba-d54b-a241e495e398"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzCiA_XwEmiZ"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.preprocess(self.dataset[index]) # токенизируем\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "      text = text.lower().replace(\"ё\", \"е\")\n",
        "      text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
        "      text = re.sub('@[^\\s]+', 'USER', text)\n",
        "      text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "      text = re.sub(' +', ' ', text)\n",
        "      return text.strip()\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_ids, y"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW5cEeWCEmls"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THcFzxf7Et1F"
      },
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ra9WCOlEwt9"
      },
      "source": [
        "ft = FastText(tweets_data['clean_text'].tolist(), size=100, window=5, min_count=1)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DuixsuiQbWB"
      },
      "source": [
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = ft.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPNKh0kaEwwq"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.bigrams2 = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n",
        "\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.relu(self.bigrams(embedded))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.relu(self.trigrams(embedded))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        bigrams = self.pooling(self.relu(self.bigrams2(concat)))\n",
        "        pooling = bigrams.max(2)[0] \n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP8TSRmJEw0G"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7F-Dv8YHkzp"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAwfCJao8di7"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (words, ys) in enumerate(iterator): \n",
        "            for word in model(words):\n",
        "                preds.append(word.cpu().detach().numpy().round())\n",
        "    return preds"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMcm6jDYHk17"
      },
      "source": [
        "model = CNN(len(word2id), 2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua9NAIe6HwFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d004de-1b3b-4697-e400-287b69421076"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.735472496598959\n",
            "Train loss: 0.7095405712272181\n",
            "Train loss: 0.7002316045761109\n",
            "Train loss: 0.6947283059803407\n",
            "Train loss: 0.6913344640107382\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.714620940387249, Val f1: 0.6434759497642517\n",
            "Val loss: 0.6927909598206029, Val f1: 0.6230840086936951\n",
            "Val loss: 0.6863144278526306, Val f1: 0.6149309873580933\n",
            "Val loss: 0.6827635702802174, Val f1: 0.6135111451148987\n",
            "Val loss: 0.680903964809009, Val f1: 0.6111968755722046\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.351104736328125, Val f1: 1.2278809547424316\n",
            "Val loss: 0.8975805640220642, Val f1: 0.8272130489349365\n",
            "Val loss: 0.8083777785301208, Val f1: 0.7333551645278931\n",
            "Val loss: 0.7702023472104754, Val f1: 0.69706130027771\n",
            "Val loss: 0.7480032642682394, Val f1: 0.6762056946754456\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.7149573117494583\n",
            "Train loss: 0.6912631952401364\n",
            "Train loss: 0.6827675044536591\n",
            "Train loss: 0.6788813703095735\n",
            "Train loss: 0.6755706405355817\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7037234380841255, Val f1: 0.5763689279556274\n",
            "Val loss: 0.6807107112624429, Val f1: 0.5649564266204834\n",
            "Val loss: 0.6744623029232025, Val f1: 0.5574795007705688\n",
            "Val loss: 0.6707727668890312, Val f1: 0.555805504322052\n",
            "Val loss: 0.6684151107356662, Val f1: 0.5554230809211731\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3319194912910461, Val f1: 1.1082656383514404\n",
            "Val loss: 0.8837420145670573, Val f1: 0.7548327445983887\n",
            "Val loss: 0.795234227180481, Val f1: 0.6674729585647583\n",
            "Val loss: 0.7584266662597656, Val f1: 0.6320423483848572\n",
            "Val loss: 0.735980298784044, Val f1: 0.6132612228393555\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6994966417551041\n",
            "Train loss: 0.6758727955095696\n",
            "Train loss: 0.6686294865608216\n",
            "Train loss: 0.6648295326019401\n",
            "Train loss: 0.6612572833186104\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6828911155462265, Val f1: 0.6802772879600525\n",
            "Val loss: 0.6625766248414011, Val f1: 0.6610119342803955\n",
            "Val loss: 0.6556668841838836, Val f1: 0.6537904739379883\n",
            "Val loss: 0.6530658086734031, Val f1: 0.6488699316978455\n",
            "Val loss: 0.6513635438112986, Val f1: 0.6467106938362122\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2961353063583374, Val f1: 1.315812110900879\n",
            "Val loss: 0.8598699371019999, Val f1: 0.8840346336364746\n",
            "Val loss: 0.7755114316940308, Val f1: 0.7818931937217712\n",
            "Val loss: 0.7397115656307766, Val f1: 0.7407614588737488\n",
            "Val loss: 0.7180427643987868, Val f1: 0.7155840992927551\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6821241639554501\n",
            "Train loss: 0.6624496181805929\n",
            "Train loss: 0.6552988684177399\n",
            "Train loss: 0.6509705105824257\n",
            "Train loss: 0.6476526991242454\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.668268758803606, Val f1: 0.6781298518180847\n",
            "Val loss: 0.6498156280228586, Val f1: 0.6554393768310547\n",
            "Val loss: 0.6444273698329925, Val f1: 0.6474934816360474\n",
            "Val loss: 0.641479192385033, Val f1: 0.641989529132843\n",
            "Val loss: 0.6393194049596786, Val f1: 0.6391132473945618\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.275926113128662, Val f1: 1.2874176502227783\n",
            "Val loss: 0.8475224177042643, Val f1: 0.8691433668136597\n",
            "Val loss: 0.7638169169425965, Val f1: 0.7694569826126099\n",
            "Val loss: 0.7289007646696908, Val f1: 0.7302934527397156\n",
            "Val loss: 0.7072509593433804, Val f1: 0.7029498815536499\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6724537201225758\n",
            "Train loss: 0.6509800968748151\n",
            "Train loss: 0.6427059018611908\n",
            "Train loss: 0.6395315981622952\n",
            "Train loss: 0.6361519162143979\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6621385775506496, Val f1: 0.7186697721481323\n",
            "Val loss: 0.642833209399021, Val f1: 0.6951661705970764\n",
            "Val loss: 0.6351557016372681, Val f1: 0.6902351379394531\n",
            "Val loss: 0.6315125022361527, Val f1: 0.6868537664413452\n",
            "Val loss: 0.6287028016079039, Val f1: 0.6859984993934631\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2485536336898804, Val f1: 1.387146234512329\n",
            "Val loss: 0.8308004140853882, Val f1: 0.9257914423942566\n",
            "Val loss: 0.7504646897315979, Val f1: 0.8216513991355896\n",
            "Val loss: 0.7159918035779681, Val f1: 0.7801600098609924\n",
            "Val loss: 0.6951995160844591, Val f1: 0.7541727423667908\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.6642467975616455\n",
            "Train loss: 0.6413362676447089\n",
            "Train loss: 0.6332551169395447\n",
            "Train loss: 0.6291464060100157\n",
            "Train loss: 0.6263139666545958\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6452575474977493, Val f1: 0.7395036220550537\n",
            "Val loss: 0.627372535792264, Val f1: 0.7119279503822327\n",
            "Val loss: 0.621006759405136, Val f1: 0.7037947773933411\n",
            "Val loss: 0.6203744882967934, Val f1: 0.6970896124839783\n",
            "Val loss: 0.6186054987566811, Val f1: 0.6943886876106262\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2263070344924927, Val f1: 1.4008712768554688\n",
            "Val loss: 0.8171182672182719, Val f1: 0.9350892305374146\n",
            "Val loss: 0.7382786750793457, Val f1: 0.8330277800559998\n",
            "Val loss: 0.7046257257461548, Val f1: 0.7902807593345642\n",
            "Val loss: 0.6842061546113756, Val f1: 0.7622183561325073\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.6494146585464478\n",
            "Train loss: 0.6311987512039415\n",
            "Train loss: 0.6233873283863067\n",
            "Train loss: 0.6180559708111322\n",
            "Train loss: 0.615826333562533\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6391348242759705, Val f1: 0.6778852939605713\n",
            "Val loss: 0.62107853455977, Val f1: 0.6533468961715698\n",
            "Val loss: 0.6138432800769806, Val f1: 0.6487019062042236\n",
            "Val loss: 0.6105911838474558, Val f1: 0.6455318927764893\n",
            "Val loss: 0.6090447299537205, Val f1: 0.6438549757003784\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2120229005813599, Val f1: 1.3151320219039917\n",
            "Val loss: 0.8085637887318929, Val f1: 0.8816983103752136\n",
            "Val loss: 0.7291705012321472, Val f1: 0.7754368185997009\n",
            "Val loss: 0.6971861890384129, Val f1: 0.7329844832420349\n",
            "Val loss: 0.6763161818186442, Val f1: 0.7060131430625916\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.6381630450487137\n",
            "Train loss: 0.6162203243284514\n",
            "Train loss: 0.6096534132957458\n",
            "Train loss: 0.6060054880469593\n",
            "Train loss: 0.6032752643028895\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.632913913577795, Val f1: 0.6547186374664307\n",
            "Val loss: 0.6136656710595796, Val f1: 0.6324359774589539\n",
            "Val loss: 0.6072595882415771, Val f1: 0.6294729709625244\n",
            "Val loss: 0.6042314518743487, Val f1: 0.6266676783561707\n",
            "Val loss: 0.6027603191988808, Val f1: 0.6246022582054138\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1966928243637085, Val f1: 1.282781958580017\n",
            "Val loss: 0.7996483445167542, Val f1: 0.8553115725517273\n",
            "Val loss: 0.7210974335670471, Val f1: 0.7533854246139526\n",
            "Val loss: 0.6901729277202061, Val f1: 0.7127739191055298\n",
            "Val loss: 0.6696891188621521, Val f1: 0.6853551864624023\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.625105258077383\n",
            "Train loss: 0.6076839746850909\n",
            "Train loss: 0.6012039911746979\n",
            "Train loss: 0.5974803824922932\n",
            "Train loss: 0.5946964855704989\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6176462471485138, Val f1: 0.7547480463981628\n",
            "Val loss: 0.6001225077744686, Val f1: 0.7288188934326172\n",
            "Val loss: 0.5942561006546021, Val f1: 0.7205908298492432\n",
            "Val loss: 0.5914897936493603, Val f1: 0.7170055508613586\n",
            "Val loss: 0.5909364542790821, Val f1: 0.7145721316337585\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1648064255714417, Val f1: 1.429597020149231\n",
            "Val loss: 0.7784841458002726, Val f1: 0.9542508125305176\n",
            "Val loss: 0.7055621862411499, Val f1: 0.850921630859375\n",
            "Val loss: 0.6745806932449341, Val f1: 0.8097756505012512\n",
            "Val loss: 0.6551550759209527, Val f1: 0.7820004820823669\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.6198864951729774\n",
            "Train loss: 0.5997107679193671\n",
            "Train loss: 0.5921270227432252\n",
            "Train loss: 0.5891973447443833\n",
            "Train loss: 0.5869090188117254\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6080080159008503, Val f1: 0.717265784740448\n",
            "Val loss: 0.591630838134072, Val f1: 0.6879599690437317\n",
            "Val loss: 0.5853328120708465, Val f1: 0.6820110082626343\n",
            "Val loss: 0.5839608969973095, Val f1: 0.6776981949806213\n",
            "Val loss: 0.5821250882886705, Val f1: 0.6749995350837708\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1528414487838745, Val f1: 1.3764288425445557\n",
            "Val loss: 0.7703619599342346, Val f1: 0.9143109321594238\n",
            "Val loss: 0.6965858221054078, Val f1: 0.810158908367157\n",
            "Val loss: 0.6666596276419503, Val f1: 0.7653566598892212\n",
            "Val loss: 0.6474077767795987, Val f1: 0.7367685437202454\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.6123435832560062\n",
            "Train loss: 0.5936639272805416\n",
            "Train loss: 0.5854998993873596\n",
            "Train loss: 0.5827230188383985\n",
            "Train loss: 0.5805819843496595\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6094559282064438, Val f1: 0.7558767199516296\n",
            "Val loss: 0.5891110644196019, Val f1: 0.7349165081977844\n",
            "Val loss: 0.5818309473991394, Val f1: 0.726530909538269\n",
            "Val loss: 0.5796285405087827, Val f1: 0.7220854163169861\n",
            "Val loss: 0.5779209761392503, Val f1: 0.7191571593284607\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1449248790740967, Val f1: 1.4229791164398193\n",
            "Val loss: 0.7637970447540283, Val f1: 0.9531695246696472\n",
            "Val loss: 0.6925924777984619, Val f1: 0.8530657887458801\n",
            "Val loss: 0.661734938621521, Val f1: 0.8102352619171143\n",
            "Val loss: 0.6429680585861206, Val f1: 0.7828998565673828\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.6059706881642342\n",
            "Train loss: 0.5868124239372484\n",
            "Train loss: 0.5813604259490966\n",
            "Train loss: 0.5777338325087704\n",
            "Train loss: 0.576107280594962\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6025771051645279, Val f1: 0.717036783695221\n",
            "Val loss: 0.5825863661188068, Val f1: 0.7026209235191345\n",
            "Val loss: 0.5758697247505188, Val f1: 0.6960211992263794\n",
            "Val loss: 0.5724148118673865, Val f1: 0.6917117238044739\n",
            "Val loss: 0.5710763682921728, Val f1: 0.6889983415603638\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1352271437644958, Val f1: 1.386584997177124\n",
            "Val loss: 0.7587225635846456, Val f1: 0.9226417541503906\n",
            "Val loss: 0.6867563605308533, Val f1: 0.8224727511405945\n",
            "Val loss: 0.657070483480181, Val f1: 0.7762624621391296\n",
            "Val loss: 0.6379197769694858, Val f1: 0.749144971370697\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.6002681702375412\n",
            "Train loss: 0.584789104533918\n",
            "Train loss: 0.5769780731201172\n",
            "Train loss: 0.5751041342963034\n",
            "Train loss: 0.5717460371199108\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5967858768999577, Val f1: 0.74312824010849\n",
            "Val loss: 0.5770437320073446, Val f1: 0.7215887308120728\n",
            "Val loss: 0.5717794346809387, Val f1: 0.7122920155525208\n",
            "Val loss: 0.5683344931744817, Val f1: 0.7107247710227966\n",
            "Val loss: 0.5666815532105309, Val f1: 0.7088220715522766\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1258163452148438, Val f1: 1.4200949668884277\n",
            "Val loss: 0.7532611687978109, Val f1: 0.9412783980369568\n",
            "Val loss: 0.6828944444656372, Val f1: 0.8394877314567566\n",
            "Val loss: 0.6529197181974139, Val f1: 0.7955088019371033\n",
            "Val loss: 0.6341244180997213, Val f1: 0.7675541043281555\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.6058879047632217\n",
            "Train loss: 0.5828205040006926\n",
            "Train loss: 0.5739261996746063\n",
            "Train loss: 0.5709715458884168\n",
            "Train loss: 0.5688254811934063\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.606800202280283, Val f1: 0.777904748916626\n",
            "Val loss: 0.5894014889543707, Val f1: 0.7525019645690918\n",
            "Val loss: 0.5839515733718872, Val f1: 0.7435663342475891\n",
            "Val loss: 0.580243706703186, Val f1: 0.7402752637863159\n",
            "Val loss: 0.5786032818612599, Val f1: 0.7384257316589355\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1437627673149109, Val f1: 1.453458309173584\n",
            "Val loss: 0.7641594608624777, Val f1: 0.969928503036499\n",
            "Val loss: 0.6946635484695435, Val f1: 0.8689669966697693\n",
            "Val loss: 0.6635292598179409, Val f1: 0.8256679177284241\n",
            "Val loss: 0.644838915930854, Val f1: 0.8002244234085083\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.5990532003343105\n",
            "Train loss: 0.5816482179092638\n",
            "Train loss: 0.5756147778034211\n",
            "Train loss: 0.5711819227062055\n",
            "Train loss: 0.5682772440569741\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5872095488011837, Val f1: 0.7658982872962952\n",
            "Val loss: 0.569842542662765, Val f1: 0.7447171807289124\n",
            "Val loss: 0.5668782043457031, Val f1: 0.7345297336578369\n",
            "Val loss: 0.5638694184929577, Val f1: 0.73041170835495\n",
            "Val loss: 0.5629776538837523, Val f1: 0.7290788292884827\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1198117136955261, Val f1: 1.4432631731033325\n",
            "Val loss: 0.7476734320322672, Val f1: 0.9633726477622986\n",
            "Val loss: 0.6789833307266235, Val f1: 0.8587956428527832\n",
            "Val loss: 0.6490961057799203, Val f1: 0.8160445094108582\n",
            "Val loss: 0.6302585668034024, Val f1: 0.7892107367515564\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.5917639918625355\n",
            "Train loss: 0.5735875115250096\n",
            "Train loss: 0.5681792187690735\n",
            "Train loss: 0.5642742390063271\n",
            "Train loss: 0.5624165790421622\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5828316435217857, Val f1: 0.7615491151809692\n",
            "Val loss: 0.5650823856845046, Val f1: 0.7370865941047668\n",
            "Val loss: 0.5617691922187805, Val f1: 0.7275282740592957\n",
            "Val loss: 0.5592930441472068, Val f1: 0.7236180305480957\n",
            "Val loss: 0.5576847514935902, Val f1: 0.7212337255477905\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.11209636926651, Val f1: 1.435877799987793\n",
            "Val loss: 0.7429450750350952, Val f1: 0.9558897018432617\n",
            "Val loss: 0.6743143677711487, Val f1: 0.8527435660362244\n",
            "Val loss: 0.6450454933302743, Val f1: 0.8080967664718628\n",
            "Val loss: 0.6263306339581808, Val f1: 0.7805744409561157\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.5902233608067036\n",
            "Train loss: 0.5697260426752495\n",
            "Train loss: 0.56395103931427\n",
            "Train loss: 0.5612206556903783\n",
            "Train loss: 0.5604036351044973\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5873407237231731, Val f1: 0.7132478356361389\n",
            "Val loss: 0.5703470562443589, Val f1: 0.6908944249153137\n",
            "Val loss: 0.5666227233409882, Val f1: 0.6804115176200867\n",
            "Val loss: 0.5622654637294029, Val f1: 0.678291916847229\n",
            "Val loss: 0.5606348365545273, Val f1: 0.6753444671630859\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.12636399269104, Val f1: 1.3576550483703613\n",
            "Val loss: 0.7530273596445719, Val f1: 0.8987188339233398\n",
            "Val loss: 0.6812547206878662, Val f1: 0.7960805892944336\n",
            "Val loss: 0.6526871919631958, Val f1: 0.7524296045303345\n",
            "Val loss: 0.6331304510434469, Val f1: 0.7264732718467712\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.5829462334513664\n",
            "Train loss: 0.5675739483399824\n",
            "Train loss: 0.5618057429790497\n",
            "Train loss: 0.5582757022843432\n",
            "Train loss: 0.5578209715230125\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5750365443527699, Val f1: 0.7404849529266357\n",
            "Val loss: 0.5576021761605234, Val f1: 0.7211519479751587\n",
            "Val loss: 0.5549970281124115, Val f1: 0.7099743485450745\n",
            "Val loss: 0.5527908837617334, Val f1: 0.7056612372398376\n",
            "Val loss: 0.5527326485940388, Val f1: 0.7021817564964294\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1137292981147766, Val f1: 1.404689073562622\n",
            "Val loss: 0.7436592777570089, Val f1: 0.9339994788169861\n",
            "Val loss: 0.6739646673202515, Val f1: 0.8294398188591003\n",
            "Val loss: 0.6449974434716361, Val f1: 0.7862101197242737\n",
            "Val loss: 0.6259357730547587, Val f1: 0.7584555149078369\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.5845745243132114\n",
            "Train loss: 0.5656197667121887\n",
            "Train loss: 0.5589716565608979\n",
            "Train loss: 0.5574864531630901\n",
            "Train loss: 0.5567010634002232\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5757180266082287, Val f1: 0.7451918125152588\n",
            "Val loss: 0.5612724802710793, Val f1: 0.7203211188316345\n",
            "Val loss: 0.5554198205471039, Val f1: 0.7154740691184998\n",
            "Val loss: 0.5516192690650029, Val f1: 0.7117962837219238\n",
            "Val loss: 0.5506609025455657, Val f1: 0.7089753746986389\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1104907989501953, Val f1: 1.4168295860290527\n",
            "Val loss: 0.7413423260052999, Val f1: 0.9373695850372314\n",
            "Val loss: 0.6714325904846191, Val f1: 0.8343760371208191\n",
            "Val loss: 0.6425125428608486, Val f1: 0.7896179556846619\n",
            "Val loss: 0.6233933236863878, Val f1: 0.7612863779067993\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.5794461444020271\n",
            "Train loss: 0.5642684499422709\n",
            "Train loss: 0.5591371750831604\n",
            "Train loss: 0.556057291244393\n",
            "Train loss: 0.5533681660890579\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5790658481419086, Val f1: 0.742868959903717\n",
            "Val loss: 0.5588377948963281, Val f1: 0.7225248217582703\n",
            "Val loss: 0.553236802816391, Val f1: 0.7151996493339539\n",
            "Val loss: 0.551475836269891, Val f1: 0.711646556854248\n",
            "Val loss: 0.5491836468378702, Val f1: 0.7085883021354675\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1072248220443726, Val f1: 1.409961462020874\n",
            "Val loss: 0.7392832438151041, Val f1: 0.9363760352134705\n",
            "Val loss: 0.6702561616897583, Val f1: 0.8359546065330505\n",
            "Val loss: 0.6414239576884678, Val f1: 0.7913811802864075\n",
            "Val loss: 0.622438391049703, Val f1: 0.7635107636451721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtRXVHYWV57N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "1a8fc3cf-2d85-4c2b-b478-6eec4d00d58b"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "plt.title('Train')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Losses')\n",
        "plt.grid()\n",
        "ax.plot(losses)\n",
        "ax.plot(losses_eval)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR8xnuH0EUqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f014009d-8d9a-423e-96bf-a6cb063f4a80"
      },
      "source": [
        "losses[-1]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.546547457575798"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_EsY3i_EUqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a574f8d-d7eb-4c44-d595-fe2f1cbb2b8b"
      },
      "source": [
        "losses_eval[-1]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5601945519447327"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw42vTwmEUqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b429c3de-bbec-4720-beaa-72e7f719377e"
      },
      "source": [
        "f1s[-1]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7003, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSU-_j3iEUqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11038d9d-ff17-4851-e31c-157527c8aac2"
      },
      "source": [
        "f1s_eval[-1]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6872, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nloTzDyQ8nf2"
      },
      "source": [
        "val_sentences['predicted']  = predict(model, val_iterator)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQcwgq9b8oHP",
        "outputId": "2a78a5f2-98a7-4291-b51d-7c247042aa24"
      },
      "source": [
        "# TP\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['здоровски погуляли по магазинам с USER пингвины',\n",
              " 'прикольно когда к тебе в бар приходит девушка выпить кофе на мин 1 15 а в итоге зависает у тебя в баре на хороших часа полтора',\n",
              " 'USER не умирай еще тебе предстоит вечер',\n",
              " 'rt USER юмор красота требует жертв придумай смешную подпись к картинке URL',\n",
              " 'давайте уж побыстрее 12 декабря объявляйте выходным днем и переносите на пятницу 13 буду строить планы',\n",
              " 'USER холодно очень и паскаль делает мне страдай d',\n",
              " 'время сна я утомилась дико завтра сложный день 6 5 часов мне на восстановление сил всем доброй ночи',\n",
              " 'USER смешно ему и плюс мне масса нужна в спорте немного я как бить буду если сил нету крч диета для идиотов',\n",
              " 'USER норм вообще то мы на наших ярмарках вообще мало зарабатываем',\n",
              " 'USER USER если ты затусишь после у сережи тебе вообще супер удобно будет']"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fus2tQvF8zHy",
        "outputId": "5f2dd7e0-2343-4383-fc10-71a698133dae"
      },
      "source": [
        "# FN\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ник вернется в сериал полиция гавайев счастьерадость ну правильно а то закончили на самом интересном',\n",
              " 'USER ну у меня времени много могу поиграть в шерлока но лучше скажи',\n",
              " 'USER иногда хочется тебя ретвитить но функция недоступна',\n",
              " 'USER USER законная власть должна в ес вступать а не рот свой открывать вот что главно',\n",
              " 'USER 27 2 7 в 7 лет в крепости у весного огня почетный караул ебнулся чтоли',\n",
              " 'я только недавно хотела зимы теперь хочу лета а у лета я буду просить зимы',\n",
              " 'на улице очень весело но очень холодно выбор между сугробом и домом не так то прост на самом деле',\n",
              " 'USER ну я пока еще с тюленем не танцевала',\n",
              " 'USER я родилась на украине но еще в детстве переехали с мамой в россию ты из какого города',\n",
              " 'USER USER да чего там две дырки проделать для рук и все х']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POlyKuwf8zRt",
        "outputId": "c72e8b67-6e28-48d9-c5dc-d179d8be881d"
      },
      "source": [
        "# FP\n",
        "val_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER ну я всегда говорил что я тп',\n",
              " 'USER дина ты дура чтоли о о заткнись чтоли о о',\n",
              " 'rt USER епт так ты баба нет кто тебе сказал такую глупость у меня есть пипирка URL',\n",
              " 'если я его потеряю я этого просто не переживу',\n",
              " 'рано утром снова кровь сдавать снова не позавтракаю',\n",
              " 'rt USER есть у кого нибудь наушники кому не жалко дайте мне',\n",
              " 'USER USER сегодня последний день и больше в этом году мы не увидим наших любимых мальчиков до самой осени',\n",
              " 'а у мну не работает рекомендация с комментами вторая попытка и фейл чяднт',\n",
              " 'сразу ощущается разница между журналистами иностранными и нашими путинотвечает',\n",
              " 'USER я так и не посмотрела фильм мне пришлось срочно уйти']"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hZbJzSDSzB8"
      },
      "source": [
        "TP: здесь в основном содержатся положительные высказывания\n",
        "\n",
        "FN: здесь в основном содержатс шуточные высказывания\n",
        "\n",
        "FP: можно заметить, что тут находятся высказывания, содержащие в основном негативный контекст, а также частица *не*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kS-iqmqqTl0"
      },
      "source": [
        "#Улучшение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhau3QZMxL-b"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r1QKRKnxL-c"
      },
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtlEwlQEqT1p"
      },
      "source": [
        "class BetterCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.bigrams_over = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n",
        "\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.relu(self.bigrams(embedded))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.relu(self.trigrams(embedded))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        bigrams = self.dropout(self.pooling(self.relu(self.bigrams_over(concat))))\n",
        "        pooling = bigrams.max(2)[0] \n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "\n",
        "        logits = self.hidden(pooling)\n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lOb9INUshuu"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEfMOypqshw9"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpLd4Do9sh0h"
      },
      "source": [
        "model = BetterCNN(len(word2id), 10)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzEYQ3sE9nnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d51647-8918-47d1-bf1a-0fa29eb75ad9"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7568571642041206\n",
            "Train loss: 0.7144657030250087\n",
            "Train loss: 0.6943663966655731\n",
            "Train loss: 0.6793940796780942\n",
            "Train loss: 0.6678465831847418\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6615765579044819, Val f1: 0.7023764252662659\n",
            "Val loss: 0.6430198658596386, Val f1: 0.6805157661437988\n",
            "Val loss: 0.6372658371925354, Val f1: 0.6724695563316345\n",
            "Val loss: 0.6342689012413594, Val f1: 0.6674272418022156\n",
            "Val loss: 0.6324084230831691, Val f1: 0.6657857298851013\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.242378294467926, Val f1: 1.3243322372436523\n",
            "Val loss: 0.8301623662312826, Val f1: 0.8778156638145447\n",
            "Val loss: 0.7492467164993286, Val f1: 0.7868083119392395\n",
            "Val loss: 0.7146236300468445, Val f1: 0.7527580857276917\n",
            "Val loss: 0.6949897011121114, Val f1: 0.7273690104484558\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6520518027245998\n",
            "Train loss: 0.6283752069328771\n",
            "Train loss: 0.6211705350875855\n",
            "Train loss: 0.6168573178462128\n",
            "Train loss: 0.613216431367965\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6458890624344349, Val f1: 0.7596393823623657\n",
            "Val loss: 0.6258088765722333, Val f1: 0.7350422143936157\n",
            "Val loss: 0.6199139964580536, Val f1: 0.7269172668457031\n",
            "Val loss: 0.6166895236541976, Val f1: 0.7223744988441467\n",
            "Val loss: 0.6150748644556318, Val f1: 0.7189284563064575\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2027852535247803, Val f1: 1.4402318000793457\n",
            "Val loss: 0.8048340876897176, Val f1: 0.963676929473877\n",
            "Val loss: 0.7290831208229065, Val f1: 0.8583086133003235\n",
            "Val loss: 0.6950264402798244, Val f1: 0.8137891888618469\n",
            "Val loss: 0.6765710645251803, Val f1: 0.7853044867515564\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6332346796989441\n",
            "Train loss: 0.6118433619990493\n",
            "Train loss: 0.603011566400528\n",
            "Train loss: 0.59868137783079\n",
            "Train loss: 0.5962579874765306\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6288027614355087, Val f1: 0.7339417934417725\n",
            "Val loss: 0.6065522179459081, Val f1: 0.7128063440322876\n",
            "Val loss: 0.5999222409725189, Val f1: 0.7081558704376221\n",
            "Val loss: 0.5966768033468901, Val f1: 0.706562340259552\n",
            "Val loss: 0.5949323163146064, Val f1: 0.7041816711425781\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1713104248046875, Val f1: 1.4136627912521362\n",
            "Val loss: 0.7827866077423096, Val f1: 0.9431027770042419\n",
            "Val loss: 0.7083305954933167, Val f1: 0.8349846005439758\n",
            "Val loss: 0.6757762942995343, Val f1: 0.796486496925354\n",
            "Val loss: 0.6568959024217393, Val f1: 0.7683248519897461\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6167097315192223\n",
            "Train loss: 0.5958273627541282\n",
            "Train loss: 0.5899611675739288\n",
            "Train loss: 0.5858167027359578\n",
            "Train loss: 0.5834780236085256\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6143605373799801, Val f1: 0.759007453918457\n",
            "Val loss: 0.5954914652939999, Val f1: 0.7374216318130493\n",
            "Val loss: 0.5904432463645936, Val f1: 0.7295870780944824\n",
            "Val loss: 0.5873152124347971, Val f1: 0.7256579995155334\n",
            "Val loss: 0.5858342739797774, Val f1: 0.7234084010124207\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1564511060714722, Val f1: 1.4283084869384766\n",
            "Val loss: 0.7719178001085917, Val f1: 0.9555478096008301\n",
            "Val loss: 0.6983874201774597, Val f1: 0.8546862602233887\n",
            "Val loss: 0.6666803019387382, Val f1: 0.81409752368927\n",
            "Val loss: 0.6474880973498026, Val f1: 0.788940966129303\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6054941564798355\n",
            "Train loss: 0.5867856813199592\n",
            "Train loss: 0.5806254053115845\n",
            "Train loss: 0.5755925267489989\n",
            "Train loss: 0.5722819105500266\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6024460308253765, Val f1: 0.7652328014373779\n",
            "Val loss: 0.5835948929642186, Val f1: 0.7416779398918152\n",
            "Val loss: 0.5784753096103669, Val f1: 0.7323734760284424\n",
            "Val loss: 0.5758542569715586, Val f1: 0.7282917499542236\n",
            "Val loss: 0.5740661684955869, Val f1: 0.7259569764137268\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1347315907478333, Val f1: 1.4400211572647095\n",
            "Val loss: 0.7564733624458313, Val f1: 0.9617751836776733\n",
            "Val loss: 0.6847051978111267, Val f1: 0.8576120734214783\n",
            "Val loss: 0.6543151395661491, Val f1: 0.8152582049369812\n",
            "Val loss: 0.6355699565675523, Val f1: 0.7906054854393005\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5941104926168919\n",
            "Train loss: 0.5758683500867902\n",
            "Train loss: 0.5703940522670746\n",
            "Train loss: 0.5670938135972664\n",
            "Train loss: 0.5667057058640889\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5939165242016315, Val f1: 0.7701416611671448\n",
            "Val loss: 0.574903237097191, Val f1: 0.7456552982330322\n",
            "Val loss: 0.5703921520709991, Val f1: 0.7365225553512573\n",
            "Val loss: 0.5678390051001934, Val f1: 0.7316911816596985\n",
            "Val loss: 0.5664456337690353, Val f1: 0.7282513976097107\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1249298453330994, Val f1: 1.4487898349761963\n",
            "Val loss: 0.7494527896245321, Val f1: 0.9682571291923523\n",
            "Val loss: 0.6772751092910767, Val f1: 0.8639551401138306\n",
            "Val loss: 0.6476098554474967, Val f1: 0.8203709125518799\n",
            "Val loss: 0.6284994085629781, Val f1: 0.7946915626525879\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5884831137955189\n",
            "Train loss: 0.5713865991794702\n",
            "Train loss: 0.5647276771068573\n",
            "Train loss: 0.5618122620369072\n",
            "Train loss: 0.5580226842846189\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5948397219181061, Val f1: 0.7864964008331299\n",
            "Val loss: 0.5767797014930032, Val f1: 0.764060914516449\n",
            "Val loss: 0.5714818406105041, Val f1: 0.7577738761901855\n",
            "Val loss: 0.5679391088770397, Val f1: 0.7558141946792603\n",
            "Val loss: 0.5668439865112305, Val f1: 0.7531701326370239\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.121593952178955, Val f1: 1.4898442029953003\n",
            "Val loss: 0.7465104262034098, Val f1: 0.995357871055603\n",
            "Val loss: 0.6764981627464295, Val f1: 0.887961208820343\n",
            "Val loss: 0.6465256554739816, Val f1: 0.8431943655014038\n",
            "Val loss: 0.6278537842962477, Val f1: 0.8181065320968628\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5783159360289574\n",
            "Train loss: 0.5634709885626128\n",
            "Train loss: 0.5569711565971375\n",
            "Train loss: 0.554768157539083\n",
            "Train loss: 0.5520622318699246\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5846313759684563, Val f1: 0.7889881134033203\n",
            "Val loss: 0.5668492371385748, Val f1: 0.763920247554779\n",
            "Val loss: 0.5613819026947021, Val f1: 0.7561450004577637\n",
            "Val loss: 0.5582479452019307, Val f1: 0.7534080147743225\n",
            "Val loss: 0.556041711143085, Val f1: 0.7523993253707886\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.103525459766388, Val f1: 1.4863178730010986\n",
            "Val loss: 0.7346369028091431, Val f1: 0.9929110407829285\n",
            "Val loss: 0.6654916524887085, Val f1: 0.887794017791748\n",
            "Val loss: 0.6362274714878627, Val f1: 0.8448315858840942\n",
            "Val loss: 0.61797077788247, Val f1: 0.8181368708610535\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5827251933515072\n",
            "Train loss: 0.5640951467282844\n",
            "Train loss: 0.5559096848964691\n",
            "Train loss: 0.5528941990724251\n",
            "Train loss: 0.550488646541323\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5807027332484722, Val f1: 0.7885677218437195\n",
            "Val loss: 0.5624544999816201, Val f1: 0.7657781839370728\n",
            "Val loss: 0.556315336227417, Val f1: 0.7582235336303711\n",
            "Val loss: 0.5536943860908052, Val f1: 0.7551730871200562\n",
            "Val loss: 0.5517090324844632, Val f1: 0.7541588544845581\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0989342331886292, Val f1: 1.5008866786956787\n",
            "Val loss: 0.7304869095484415, Val f1: 1.0005874633789062\n",
            "Val loss: 0.6608866810798645, Val f1: 0.8920113444328308\n",
            "Val loss: 0.6323351263999939, Val f1: 0.8467411398887634\n",
            "Val loss: 0.6140657530890571, Val f1: 0.820374071598053\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5752904042601585\n",
            "Train loss: 0.5549064296664614\n",
            "Train loss: 0.5487780237197876\n",
            "Train loss: 0.5450640641041656\n",
            "Train loss: 0.5466837932666143\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5715496800839901, Val f1: 0.7580423951148987\n",
            "Val loss: 0.5564889925898928, Val f1: 0.7352332472801208\n",
            "Val loss: 0.5511443674564361, Val f1: 0.7268251776695251\n",
            "Val loss: 0.54842798033757, Val f1: 0.7231926918029785\n",
            "Val loss: 0.5466633325531369, Val f1: 0.7218212485313416\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0950642824172974, Val f1: 1.4298107624053955\n",
            "Val loss: 0.7289606928825378, Val f1: 0.9513465762138367\n",
            "Val loss: 0.6577433943748474, Val f1: 0.8470350503921509\n",
            "Val loss: 0.6301661048616681, Val f1: 0.8028002381324768\n",
            "Val loss: 0.6109351648224725, Val f1: 0.7778991460800171\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.5764215476810932\n",
            "Train loss: 0.5536922324787487\n",
            "Train loss: 0.5474076616764069\n",
            "Train loss: 0.5448455152226918\n",
            "Train loss: 0.5431390780778158\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5719456672668457, Val f1: 0.8005314469337463\n",
            "Val loss: 0.5566588856957175, Val f1: 0.773568868637085\n",
            "Val loss: 0.5506477594375611, Val f1: 0.7658472061157227\n",
            "Val loss: 0.5475658293980271, Val f1: 0.7624768614768982\n",
            "Val loss: 0.5452867391563597, Val f1: 0.7612794637680054\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0878010988235474, Val f1: 1.5039706230163574\n",
            "Val loss: 0.7231851617495219, Val f1: 1.0041134357452393\n",
            "Val loss: 0.6552610635757447, Val f1: 0.8999198079109192\n",
            "Val loss: 0.6271198987960815, Val f1: 0.8536979556083679\n",
            "Val loss: 0.6090096566412184, Val f1: 0.8268717527389526\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.5643309354782104\n",
            "Train loss: 0.546438885457588\n",
            "Train loss: 0.5441185915470124\n",
            "Train loss: 0.5406113311425963\n",
            "Train loss: 0.5388433702644848\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5747016295790672, Val f1: 0.8047412633895874\n",
            "Val loss: 0.5567087747833945, Val f1: 0.7784348726272583\n",
            "Val loss: 0.549660575389862, Val f1: 0.7716082334518433\n",
            "Val loss: 0.5465025626011749, Val f1: 0.7678309679031372\n",
            "Val loss: 0.545096334247362, Val f1: 0.7650132179260254\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0885384678840637, Val f1: 1.5103652477264404\n",
            "Val loss: 0.7231664856274923, Val f1: 1.0111016035079956\n",
            "Val loss: 0.6552931785583496, Val f1: 0.9050931930541992\n",
            "Val loss: 0.6266005464962551, Val f1: 0.8585724830627441\n",
            "Val loss: 0.6086268226305643, Val f1: 0.8329966068267822\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.5626460257917643\n",
            "Train loss: 0.5514295850739335\n",
            "Train loss: 0.5461190849542618\n",
            "Train loss: 0.5439622629044661\n",
            "Train loss: 0.5418620176968121\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5632946938276291, Val f1: 0.787568211555481\n",
            "Val loss: 0.5472730325930046, Val f1: 0.7595056891441345\n",
            "Val loss: 0.5399250864982605, Val f1: 0.7563336491584778\n",
            "Val loss: 0.5385994448590634, Val f1: 0.7502567768096924\n",
            "Val loss: 0.5366799881060919, Val f1: 0.7481029629707336\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0777652263641357, Val f1: 1.4524005651474\n",
            "Val loss: 0.7168137033780416, Val f1: 0.9776136875152588\n",
            "Val loss: 0.647974944114685, Val f1: 0.8726534247398376\n",
            "Val loss: 0.6201374105044773, Val f1: 0.8282278776168823\n",
            "Val loss: 0.6015741361512078, Val f1: 0.8034767508506775\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.5545645356178284\n",
            "Train loss: 0.540484728235187\n",
            "Train loss: 0.5353923940658569\n",
            "Train loss: 0.5326240116090917\n",
            "Train loss: 0.5309441267024904\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5637459978461266, Val f1: 0.7944076061248779\n",
            "Val loss: 0.5450186386252894, Val f1: 0.7731461524963379\n",
            "Val loss: 0.5391656577587127, Val f1: 0.7669779062271118\n",
            "Val loss: 0.5362354072172251, Val f1: 0.76361483335495\n",
            "Val loss: 0.5336809477635792, Val f1: 0.7633581757545471\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0738651156425476, Val f1: 1.4970148801803589\n",
            "Val loss: 0.7134427030881246, Val f1: 1.0011484622955322\n",
            "Val loss: 0.6452091097831726, Val f1: 0.8967043161392212\n",
            "Val loss: 0.6170381903648376, Val f1: 0.8513217568397522\n",
            "Val loss: 0.598899331357744, Val f1: 0.8245949745178223\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.5542890690267086\n",
            "Train loss: 0.5385431791796829\n",
            "Train loss: 0.5333121144771575\n",
            "Train loss: 0.5321333132573028\n",
            "Train loss: 0.5311194204148793\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5581066533923149, Val f1: 0.7702333331108093\n",
            "Val loss: 0.5408179922537371, Val f1: 0.7514609098434448\n",
            "Val loss: 0.5350755858421326, Val f1: 0.7464939951896667\n",
            "Val loss: 0.53310775223063, Val f1: 0.7416736483573914\n",
            "Val loss: 0.5308254048937843, Val f1: 0.7400614023208618\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0719253420829773, Val f1: 1.4430917501449585\n",
            "Val loss: 0.7124285896619161, Val f1: 0.9687603712081909\n",
            "Val loss: 0.6431833505630493, Val f1: 0.8637798428535461\n",
            "Val loss: 0.6158813238143921, Val f1: 0.820766806602478\n",
            "Val loss: 0.5967624915970696, Val f1: 0.7959863543510437\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.5527737829834223\n",
            "Train loss: 0.5379252659552025\n",
            "Train loss: 0.5322913497686386\n",
            "Train loss: 0.5306972687813774\n",
            "Train loss: 0.5297041695032801\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5526210516691208, Val f1: 0.7957742214202881\n",
            "Val loss: 0.5375963910059496, Val f1: 0.7700334787368774\n",
            "Val loss: 0.5338179367780685, Val f1: 0.7593138217926025\n",
            "Val loss: 0.5303345344849487, Val f1: 0.7581281065940857\n",
            "Val loss: 0.528781828780969, Val f1: 0.7560986876487732\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0671115517616272, Val f1: 1.4737615585327148\n",
            "Val loss: 0.7108761469523112, Val f1: 0.9859367609024048\n",
            "Val loss: 0.6423234224319458, Val f1: 0.8798847198486328\n",
            "Val loss: 0.6146756836346218, Val f1: 0.8341586589813232\n",
            "Val loss: 0.5961751739184061, Val f1: 0.8090594410896301\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.550844231620431\n",
            "Train loss: 0.53590676278779\n",
            "Train loss: 0.5283201253414154\n",
            "Train loss: 0.5268514151893445\n",
            "Train loss: 0.526058326519671\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5488647632300854, Val f1: 0.8047657608985901\n",
            "Val loss: 0.5349527109753002, Val f1: 0.7765238285064697\n",
            "Val loss: 0.5291510033607483, Val f1: 0.76823890209198\n",
            "Val loss: 0.5265720824697124, Val f1: 0.7639442682266235\n",
            "Val loss: 0.5253428178174155, Val f1: 0.7618077397346497\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0610726475715637, Val f1: 1.4790629148483276\n",
            "Val loss: 0.7052492499351501, Val f1: 0.9923998713493347\n",
            "Val loss: 0.6375877618789673, Val f1: 0.8876416087150574\n",
            "Val loss: 0.6105319431849888, Val f1: 0.8443659543991089\n",
            "Val loss: 0.5922874146037631, Val f1: 0.8186021447181702\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.5490571036934853\n",
            "Train loss: 0.5325283859715317\n",
            "Train loss: 0.5276471972465515\n",
            "Train loss: 0.5256890943690912\n",
            "Train loss: 0.5230088205564589\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.55074118450284, Val f1: 0.8172762393951416\n",
            "Val loss: 0.5362320235281279, Val f1: 0.789588451385498\n",
            "Val loss: 0.5312324452400208, Val f1: 0.7809802889823914\n",
            "Val loss: 0.5294893677554914, Val f1: 0.7759294509887695\n",
            "Val loss: 0.527032746445565, Val f1: 0.7756151556968689\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0628982186317444, Val f1: 1.5217297077178955\n",
            "Val loss: 0.706420878569285, Val f1: 1.0167663097381592\n",
            "Val loss: 0.6401230573654175, Val f1: 0.9115554690361023\n",
            "Val loss: 0.6118873698370797, Val f1: 0.8677681088447571\n",
            "Val loss: 0.5942707790268792, Val f1: 0.8403336405754089\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.5550537295639515\n",
            "Train loss: 0.5367676245443749\n",
            "Train loss: 0.5290626066923142\n",
            "Train loss: 0.5263921409400542\n",
            "Train loss: 0.5239185907301449\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5451796241104603, Val f1: 0.8124456405639648\n",
            "Val loss: 0.5305703481038412, Val f1: 0.7874570488929749\n",
            "Val loss: 0.5263108098506928, Val f1: 0.778856098651886\n",
            "Val loss: 0.5233458293907678, Val f1: 0.7771668434143066\n",
            "Val loss: 0.521421648916744, Val f1: 0.7748693823814392\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0550864934921265, Val f1: 1.5136003494262695\n",
            "Val loss: 0.7012697060902914, Val f1: 1.016326665878296\n",
            "Val loss: 0.6347316861152649, Val f1: 0.9092716574668884\n",
            "Val loss: 0.6071634207453046, Val f1: 0.8634370565414429\n",
            "Val loss: 0.5894685652520921, Val f1: 0.8360636830329895\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.5415361262857914\n",
            "Train loss: 0.5269788395274769\n",
            "Train loss: 0.5231575787067413\n",
            "Train loss: 0.5230799833340432\n",
            "Train loss: 0.5212122719912302\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5456133391708136, Val f1: 0.8092546463012695\n",
            "Val loss: 0.5279953813914097, Val f1: 0.7823649644851685\n",
            "Val loss: 0.525144789814949, Val f1: 0.7715732455253601\n",
            "Val loss: 0.5216568864103573, Val f1: 0.7681050896644592\n",
            "Val loss: 0.519508662323157, Val f1: 0.7667859792709351\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0546400547027588, Val f1: 1.5056016445159912\n",
            "Val loss: 0.7009292443593343, Val f1: 1.0012226104736328\n",
            "Val loss: 0.6331428050994873, Val f1: 0.8955947160720825\n",
            "Val loss: 0.6064128535134452, Val f1: 0.8497117161750793\n",
            "Val loss: 0.5884562134742737, Val f1: 0.8231008648872375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9weBeJTT9nwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "e96cdae4-3fbe-4eb7-941f-1817dcce3fb7"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "plt.title('Train')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Losses')\n",
        "plt.grid()\n",
        "ax.plot(losses)\n",
        "ax.plot(losses_eval)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svpgz1v5D-mE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c54b02-cc64-4ad0-a431-3e3f791d39e6"
      },
      "source": [
        "losses[-1]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5150239359248768"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdDjx6DgD-mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faef505a-7b10-40a5-c1de-d144479a2e1e"
      },
      "source": [
        "losses_eval[-1]"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5296105921268464"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7um8usX_D-mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a3a350-86e1-4d8f-e004-7d48a75873ae"
      },
      "source": [
        "f1s[-1]"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7577, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNWTl6lgD-mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed8751b-b1ca-41b4-e99f-81d74c98109b"
      },
      "source": [
        "f1s_eval[-1]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7408, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyfXdrnr85RR"
      },
      "source": [
        "val_sentences['predicted']  = predict(model, val_iterator)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aDJtxjG85RR",
        "outputId": "522e73d9-7de2-46bd-b47a-0554df167948"
      },
      "source": [
        "# TP\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['здоровски погуляли по магазинам с USER пингвины',\n",
              " 'прикольно когда к тебе в бар приходит девушка выпить кофе на мин 1 15 а в итоге зависает у тебя в баре на хороших часа полтора',\n",
              " 'USER не умирай еще тебе предстоит вечер',\n",
              " 'rt USER юмор красота требует жертв придумай смешную подпись к картинке URL',\n",
              " 'давайте уж побыстрее 12 декабря объявляйте выходным днем и переносите на пятницу 13 буду строить планы',\n",
              " 'USER холодно очень и паскаль делает мне страдай d',\n",
              " 'время сна я утомилась дико завтра сложный день 6 5 часов мне на восстановление сил всем доброй ночи',\n",
              " 'ник вернется в сериал полиция гавайев счастьерадость ну правильно а то закончили на самом интересном',\n",
              " 'USER смешно ему и плюс мне масса нужна в спорте немного я как бить буду если сил нету крч диета для идиотов',\n",
              " 'USER иногда хочется тебя ретвитить но функция недоступна']"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqyi6wim85RR",
        "outputId": "e3df7e08-c3bd-44ce-85d3-ded064e4ad8d"
      },
      "source": [
        "# FN\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER ну у меня времени много могу поиграть в шерлока но лучше скажи',\n",
              " 'я только недавно хотела зимы теперь хочу лета а у лета я буду просить зимы',\n",
              " 'на улице очень весело но очень холодно выбор между сугробом и домом не так то прост на самом деле',\n",
              " 'найти и не потерять муз и сл с филиппов новая песня написанная мной для USER URL',\n",
              " 'USER USER другим не стыдно другим все равно',\n",
              " 'блин аж н як забути цього не можу да не хочу боже так мило',\n",
              " 'USER сломался он не знаю от чего но когда на него звонишь он говорит тебе бай бай так я теперь чика со старым телефоном',\n",
              " 'сука позавчера отвалился скан у одного ricoh вчера у второго и вот наконец у третьего последнего компания принтеров решила бухнуть',\n",
              " 'хочу чтобы в этот новый год у меня дома собрались мои родственники',\n",
              " 'как быстро время идет неделю не заметила URL']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly6o2dM385RS",
        "outputId": "55ac8867-7c26-43b9-8ca9-440b9065ecb6"
      },
      "source": [
        "# FP\n",
        "val_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER ну я всегда говорил что я тп',\n",
              " 'кто нибудь зайдите вк USER USER USER',\n",
              " 'четвертый курс кошмар научная статья сюжет федеральные тесты сессии диплом его защита госы и чертовы рейтинги',\n",
              " 'что то много людей сейчас болеют выздоравливайте зайки',\n",
              " 'USER каждый одинок по своему',\n",
              " 'пошел я приберусь а то мама кричать будет',\n",
              " 'rt USER обновление твиттера крутое а контакта не очень',\n",
              " 'во мне две половины разрывается между желанием рожать детей и желанием жрать лсд и нюхать кокс с 37сантиметрового члена',\n",
              " 'USER он выдает мне вирусные сцылки',\n",
              " 'а у мну не работает рекомендация с комментами вторая попытка и фейл чяднт']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EefUJfEBU1oq"
      },
      "source": [
        "TP:  'ник вернется в сериал полиция гавайев счастьерадость ну правильно а то закончили на самом интересном' попало из FN сюда, наверное из-за *счастьерадость*, еще в конце немного поменялось, но в уелом почти то же самое, хотя и точность не сильно выросла\n",
        "\n",
        "FN: список стал поменьше, но в целом почти то же самое\n",
        "\n",
        "FP: а вот тут немного поменялось"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pokGIfYArga0"
      },
      "source": [
        "#Результаты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USL7wXYqyLW7"
      },
      "source": [
        "При добавлении Dropout равного 0.5, weight_decay равного 1e-4, смене learning rate с 0.0005 на 0.001 результат поменялся в лучшую сторону!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZtgiqmocOmN"
      },
      "source": [
        "#Dataset2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDbUJ2dPYN2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e2a197-d201-4f5c-f3bf-2e80b644fdd2"
      },
      "source": [
        "symbol_vocab = Counter()\n",
        "\n",
        "for text in tweets_data['clean_text']:\n",
        "    symbol_vocab.update(list(text))\n",
        "print('всего уникальных символов:', len(symbol_vocab))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia4kxH8wYQj4"
      },
      "source": [
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in symbol_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlPhfg8gYTAm"
      },
      "source": [
        "class WordSymbDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['clean_text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = torch.Tensor(dataset['tone'].values)\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.dataset[index].split() # токенизируем\n",
        "        word_ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        symbols = list(self.dataset[index])\n",
        "        symb_ids = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return word_ids, symb_ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      word_ids, symb_ids, y = list(zip(*batch))\n",
        "      padded_words = pad_sequence(word_ids, batch_first=True).to(self.device)\n",
        "      padded_symbs = pad_sequence(symb_ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_words, padded_symbs, y"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5uaF3kzYXyQ"
      },
      "source": [
        "train_dataset = WordSymbDataset(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVhgy47nYYhF"
      },
      "source": [
        "val_dataset = WordSymbDataset(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_mmzLa7YYnT"
      },
      "source": [
        "class WordSymbCNN(nn.Module):\n",
        "    def __init__(self, symb_vocab_size, symb_embedding_dim, word_vocab_size):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(word_vocab_size, 100)\n",
        "        self.word_embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.word_hidden = nn.Linear(100, 100) \n",
        "\n",
        "        self.symb_embedding = nn.Embedding(symb_vocab_size, symb_embedding_dim)\n",
        "        self.symb_bigrams = nn.Conv1d(in_channels=symb_embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.symb_trigrams = nn.Conv1d(in_channels=symb_embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.symb_pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.symb_hidden = nn.Linear(in_features=180, out_features=100)\n",
        "        \n",
        "        self.linear = nn.Linear(in_features=200, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()   \n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, words_seq, symb_seq):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.symb_embedding(symb_seq)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.dropout(self.symb_pooling(self.relu(self.symb_bigrams(embedded))))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.dropout(self.symb_pooling(self.relu(self.symb_trigrams(embedded))))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        # batch_size x filter_count2\n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "        concat = torch.cat((pooling1, pooling2), 1)\n",
        "        # batch_size x (filter_count2 + filter_count3)\n",
        "        symb_vec = self.symb_hidden(concat)\n",
        "\n",
        "        embedded_words = self.word_embedding(words_seq)   # переводим последовательность индексов в последовательность эмбеддингов\n",
        "        mean_emb_words = torch.mean(embedded_words, dim=1) # считаем средний эмбеддинг предложения\n",
        "        word_vec = self.dropout(self.word_hidden(mean_emb_words)) # пропускаем эмбеддинг через полносвязный слой \n",
        "        word_vec = self.dropout(self.relu(word_vec))\n",
        "        concat = torch.cat((symb_vec, word_vec), 1)\n",
        "        logits = self.out(self.linear(concat))      \n",
        "        return logits"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVN6g61j8Dp5"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, symbols, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts, symbols)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdgz_yPz8Dsu"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbols)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RObgaScBMs_z"
      },
      "source": [
        "def predict_word_symbol(model, iterator):\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (words, symbols, ys) in enumerate(iterator): \n",
        "            for word in model(words, symbols):\n",
        "                preds.append(word.cpu().detach().numpy().round())  # делаем предсказания на тесте \n",
        "    return preds"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvD6Y8UE8DwN"
      },
      "source": [
        "model = WordSymbCNN(len(symbol2id), 10, len(word2id))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr4DEmyR_Dr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46634eea-9656-4459-f0a6-e879a9b898a6"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.783716194331646\n",
            "Train loss: 0.7341999881195299\n",
            "Train loss: 0.7133740329742432\n",
            "Train loss: 0.7001286780656274\n",
            "Train loss: 0.6922966880457742\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.705167967826128, Val f1: 0.730217695236206\n",
            "Val loss: 0.6844959385467299, Val f1: 0.7066564559936523\n",
            "Val loss: 0.6786170685291291, Val f1: 0.6972141861915588\n",
            "Val loss: 0.6751306661918982, Val f1: 0.695022702217102\n",
            "Val loss: 0.6728183541979108, Val f1: 0.6937280297279358\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3104236721992493, Val f1: 1.4074349403381348\n",
            "Val loss: 0.8763918479283651, Val f1: 0.9372596740722656\n",
            "Val loss: 0.7937811970710754, Val f1: 0.8293129801750183\n",
            "Val loss: 0.7570397257804871, Val f1: 0.7869834303855896\n",
            "Val loss: 0.7364191545380486, Val f1: 0.7638124823570251\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6936241686344147\n",
            "Train loss: 0.668288812492833\n",
            "Train loss: 0.657392715215683\n",
            "Train loss: 0.6501845076902589\n",
            "Train loss: 0.6445250461498896\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6666521728038788, Val f1: 0.7485166788101196\n",
            "Val loss: 0.6444242903680513, Val f1: 0.7284824848175049\n",
            "Val loss: 0.638901777267456, Val f1: 0.7217282056808472\n",
            "Val loss: 0.6360101762102611, Val f1: 0.7182744741439819\n",
            "Val loss: 0.6340105271055585, Val f1: 0.7167354822158813\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2364487648010254, Val f1: 1.4459316730499268\n",
            "Val loss: 0.8299591342608134, Val f1: 0.9538065791130066\n",
            "Val loss: 0.7519160866737366, Val f1: 0.8508824706077576\n",
            "Val loss: 0.7175369688442775, Val f1: 0.8098867535591125\n",
            "Val loss: 0.6965751780403985, Val f1: 0.7887402772903442\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.649168074131012\n",
            "Train loss: 0.6276815190459742\n",
            "Train loss: 0.6186760759353638\n",
            "Train loss: 0.6116161568840938\n",
            "Train loss: 0.6071460864373616\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6329631209373474, Val f1: 0.7727938294410706\n",
            "Val loss: 0.6119613195910598, Val f1: 0.7534533143043518\n",
            "Val loss: 0.6058707165718079, Val f1: 0.7452433705329895\n",
            "Val loss: 0.602647902360603, Val f1: 0.7409352660179138\n",
            "Val loss: 0.5995424389839172, Val f1: 0.7399694919586182\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.170511245727539, Val f1: 1.4945268630981445\n",
            "Val loss: 0.7888273000717163, Val f1: 0.9818164706230164\n",
            "Val loss: 0.7165729761123657, Val f1: 0.8760183453559875\n",
            "Val loss: 0.6841038124901908, Val f1: 0.8332294821739197\n",
            "Val loss: 0.6634162995550368, Val f1: 0.8106277585029602\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6100334450602531\n",
            "Train loss: 0.5872683543147463\n",
            "Train loss: 0.5774560415744782\n",
            "Train loss: 0.5727968598479656\n",
            "Train loss: 0.5687963359412693\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5728290639817715, Val f1: 0.787243664264679\n",
            "Val loss: 0.5548353357748552, Val f1: 0.7654857635498047\n",
            "Val loss: 0.5505792105197906, Val f1: 0.757615864276886\n",
            "Val loss: 0.5479259917985148, Val f1: 0.7518867254257202\n",
            "Val loss: 0.545545565940085, Val f1: 0.7499831914901733\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0887488722801208, Val f1: 1.483740210533142\n",
            "Val loss: 0.733171542485555, Val f1: 0.9845401644706726\n",
            "Val loss: 0.6636056184768677, Val f1: 0.8767523169517517\n",
            "Val loss: 0.6346628580774579, Val f1: 0.8314324617385864\n",
            "Val loss: 0.6132008499569364, Val f1: 0.8119862675666809\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5680534094572067\n",
            "Train loss: 0.5534285830728936\n",
            "Train loss: 0.5449668288230896\n",
            "Train loss: 0.5398340945813194\n",
            "Train loss: 0.5345582067966461\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5337234828621149, Val f1: 0.8222634196281433\n",
            "Val loss: 0.5172619422276815, Val f1: 0.7925541400909424\n",
            "Val loss: 0.513838170170784, Val f1: 0.7818506956100464\n",
            "Val loss: 0.5124214104752043, Val f1: 0.7780113816261292\n",
            "Val loss: 0.511945594279539, Val f1: 0.7743287086486816\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0383171439170837, Val f1: 1.517733097076416\n",
            "Val loss: 0.6996612946192423, Val f1: 1.0044262409210205\n",
            "Val loss: 0.6339155197143554, Val f1: 0.8999086618423462\n",
            "Val loss: 0.60648227589471, Val f1: 0.8524363040924072\n",
            "Val loss: 0.5855256716410319, Val f1: 0.8322269320487976\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5368763692677021\n",
            "Train loss: 0.5231132904688517\n",
            "Train loss: 0.5148513507843018\n",
            "Train loss: 0.5088070404173722\n",
            "Train loss: 0.508095013598601\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5204704999923706, Val f1: 0.8264310359954834\n",
            "Val loss: 0.5022352157217084, Val f1: 0.8046156167984009\n",
            "Val loss: 0.4953118759393692, Val f1: 0.7981550693511963\n",
            "Val loss: 0.4926991209165374, Val f1: 0.7942845225334167\n",
            "Val loss: 0.4917289497596877, Val f1: 0.7924174666404724\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.017116129398346, Val f1: 1.5394110679626465\n",
            "Val loss: 0.6841861406962076, Val f1: 1.0214035511016846\n",
            "Val loss: 0.6196705222129821, Val f1: 0.9146613478660583\n",
            "Val loss: 0.5925534112112862, Val f1: 0.8677272796630859\n",
            "Val loss: 0.5720921092563205, Val f1: 0.8457852602005005\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5119784511625767\n",
            "Train loss: 0.4951734578970707\n",
            "Train loss: 0.4926939582824707\n",
            "Train loss: 0.4877398983756108\n",
            "Train loss: 0.48457426116580055\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.48756621964275837, Val f1: 0.8464950919151306\n",
            "Val loss: 0.4707686052177892, Val f1: 0.8215776681900024\n",
            "Val loss: 0.46710109055042265, Val f1: 0.811751663684845\n",
            "Val loss: 0.4650637182726789, Val f1: 0.8074287176132202\n",
            "Val loss: 0.46261540126232875, Val f1: 0.8055263161659241\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9897088408470154, Val f1: 1.5438354015350342\n",
            "Val loss: 0.6637460788091024, Val f1: 1.0290628671646118\n",
            "Val loss: 0.6001176476478577, Val f1: 0.9218925833702087\n",
            "Val loss: 0.5740419030189514, Val f1: 0.8747740983963013\n",
            "Val loss: 0.5534536606735654, Val f1: 0.8502805829048157\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.4938017576932907\n",
            "Train loss: 0.47888429417754663\n",
            "Train loss: 0.4702333867549896\n",
            "Train loss: 0.4661951327501838\n",
            "Train loss: 0.4636474833601997\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.47637028619647026, Val f1: 0.8551988005638123\n",
            "Val loss: 0.46156011657281354, Val f1: 0.8300617337226868\n",
            "Val loss: 0.4572644925117493, Val f1: 0.8215938210487366\n",
            "Val loss: 0.4548830870372146, Val f1: 0.8169853687286377\n",
            "Val loss: 0.45345577952407656, Val f1: 0.8142973780632019\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9964998960494995, Val f1: 1.547156810760498\n",
            "Val loss: 0.6662091811498007, Val f1: 1.0336766242980957\n",
            "Val loss: 0.6028329491615295, Val f1: 0.9252174496650696\n",
            "Val loss: 0.5760501878602164, Val f1: 0.8780781626701355\n",
            "Val loss: 0.555675595998764, Val f1: 0.8549236059188843\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.46678501926362514\n",
            "Train loss: 0.45162652478073584\n",
            "Train loss: 0.44689044415950774\n",
            "Train loss: 0.4446879122684251\n",
            "Train loss: 0.4433410990805853\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.45524629205465317, Val f1: 0.8648219108581543\n",
            "Val loss: 0.4425440322269093, Val f1: 0.838713526725769\n",
            "Val loss: 0.4375999057292938, Val f1: 0.8308870792388916\n",
            "Val loss: 0.4354714662281435, Val f1: 0.8261935710906982\n",
            "Val loss: 0.43487627023742315, Val f1: 0.8241114020347595\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9927752017974854, Val f1: 1.5551257133483887\n",
            "Val loss: 0.6608258386452993, Val f1: 1.0381628274917603\n",
            "Val loss: 0.5978298962116242, Val f1: 0.929841160774231\n",
            "Val loss: 0.5709504612854549, Val f1: 0.8815276026725769\n",
            "Val loss: 0.550645606385337, Val f1: 0.8570734858512878\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.45401212200522423\n",
            "Train loss: 0.4412249678915197\n",
            "Train loss: 0.4354470533132553\n",
            "Train loss: 0.4314543213417281\n",
            "Train loss: 0.43005309502283734\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4293816201388836, Val f1: 0.8797253966331482\n",
            "Val loss: 0.420040229956309, Val f1: 0.849522590637207\n",
            "Val loss: 0.41562690794467927, Val f1: 0.8412144780158997\n",
            "Val loss: 0.41278224338346453, Val f1: 0.8377799391746521\n",
            "Val loss: 0.41075406613804044, Val f1: 0.8362836241722107\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9768560826778412, Val f1: 1.5458245277404785\n",
            "Val loss: 0.6476619640986124, Val f1: 1.034818410873413\n",
            "Val loss: 0.5851079106330872, Val f1: 0.9276677966117859\n",
            "Val loss: 0.5591913035937718, Val f1: 0.8808702230453491\n",
            "Val loss: 0.5388009912437863, Val f1: 0.8554821014404297\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.4423802439123392\n",
            "Train loss: 0.429331665689295\n",
            "Train loss: 0.4237920796871185\n",
            "Train loss: 0.41872057452130673\n",
            "Train loss: 0.4155672455117816\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4090757239609957, Val f1: 0.8915889859199524\n",
            "Val loss: 0.4027107186389692, Val f1: 0.8595971465110779\n",
            "Val loss: 0.3988505035638809, Val f1: 0.8518170714378357\n",
            "Val loss: 0.39590618726032883, Val f1: 0.8473052382469177\n",
            "Val loss: 0.3956547624298504, Val f1: 0.8439507484436035\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.984870046377182, Val f1: 1.5557682514190674\n",
            "Val loss: 0.6503189504146576, Val f1: 1.0372815132141113\n",
            "Val loss: 0.5884629726409912, Val f1: 0.9298507571220398\n",
            "Val loss: 0.5620415551321847, Val f1: 0.8836801648139954\n",
            "Val loss: 0.5411499241987864, Val f1: 0.8575443029403687\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.4211396146565676\n",
            "Train loss: 0.4119340885769237\n",
            "Train loss: 0.4036715656518936\n",
            "Train loss: 0.40273313202075106\n",
            "Train loss: 0.4027090047796567\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40316499024629593, Val f1: 0.8898748159408569\n",
            "Val loss: 0.3928925530477004, Val f1: 0.8632252812385559\n",
            "Val loss: 0.38800340592861177, Val f1: 0.8551241159439087\n",
            "Val loss: 0.38450726110543776, Val f1: 0.8521403670310974\n",
            "Val loss: 0.38471140889894395, Val f1: 0.8490118384361267\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9937323927879333, Val f1: 1.5492966175079346\n",
            "Val loss: 0.654382566610972, Val f1: 1.0352332592010498\n",
            "Val loss: 0.5920266211032867, Val f1: 0.9263702630996704\n",
            "Val loss: 0.5650337764195034, Val f1: 0.8816200494766235\n",
            "Val loss: 0.5445155004660288, Val f1: 0.8560668230056763\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.4124669749289751\n",
            "Train loss: 0.3971415875536023\n",
            "Train loss: 0.39429559409618375\n",
            "Train loss: 0.39305632594806045\n",
            "Train loss: 0.39325704319136484\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.390859704464674, Val f1: 0.9019321203231812\n",
            "Val loss: 0.3804208699500922, Val f1: 0.8726603984832764\n",
            "Val loss: 0.37914188802242277, Val f1: 0.8610678911209106\n",
            "Val loss: 0.37520387457377874, Val f1: 0.858180820941925\n",
            "Val loss: 0.37382458753529046, Val f1: 0.8554801940917969\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9926559031009674, Val f1: 1.553121566772461\n",
            "Val loss: 0.6520242989063263, Val f1: 1.035796046257019\n",
            "Val loss: 0.5905664086341857, Val f1: 0.9258586168289185\n",
            "Val loss: 0.5639401631695884, Val f1: 0.8795876502990723\n",
            "Val loss: 0.5436488323741488, Val f1: 0.8542084693908691\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.40314942970871925\n",
            "Train loss: 0.3908788137363665\n",
            "Train loss: 0.38615229189395905\n",
            "Train loss: 0.3847753619080159\n",
            "Train loss: 0.3841075219568752\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38348708860576153, Val f1: 0.9010864496231079\n",
            "Val loss: 0.3732907058614673, Val f1: 0.8740780353546143\n",
            "Val loss: 0.3677066624164581, Val f1: 0.8674865365028381\n",
            "Val loss: 0.36499048346903784, Val f1: 0.8640825152397156\n",
            "Val loss: 0.36435170258794514, Val f1: 0.8604245185852051\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0157113075256348, Val f1: 1.5544893741607666\n",
            "Val loss: 0.6659144163131714, Val f1: 1.0348517894744873\n",
            "Val loss: 0.6041523098945618, Val f1: 0.925146222114563\n",
            "Val loss: 0.5764354041644505, Val f1: 0.8795492053031921\n",
            "Val loss: 0.5558883680237664, Val f1: 0.8534157872200012\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.3914893660694361\n",
            "Train loss: 0.37982576995184925\n",
            "Train loss: 0.3770718687772751\n",
            "Train loss: 0.37560550979713897\n",
            "Train loss: 0.37518253390278133\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.387822974473238, Val f1: 0.9003472328186035\n",
            "Val loss: 0.3785600011998957, Val f1: 0.8736677169799805\n",
            "Val loss: 0.37348885655403136, Val f1: 0.8652960062026978\n",
            "Val loss: 0.37232167729690896, Val f1: 0.8606042861938477\n",
            "Val loss: 0.37047662763368516, Val f1: 0.8585341572761536\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0428549647331238, Val f1: 1.5589072704315186\n",
            "Val loss: 0.6840884486834208, Val f1: 1.035309076309204\n",
            "Val loss: 0.6218449354171753, Val f1: 0.9226213693618774\n",
            "Val loss: 0.5928620270320347, Val f1: 0.8786776661872864\n",
            "Val loss: 0.572650843196445, Val f1: 0.8536737561225891\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.3805589359253645\n",
            "Train loss: 0.36837713555856183\n",
            "Train loss: 0.3700839430093765\n",
            "Train loss: 0.3671070564149031\n",
            "Train loss: 0.3662218877247402\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36254831962287426, Val f1: 0.9099403619766235\n",
            "Val loss: 0.3548461406519919, Val f1: 0.8827512264251709\n",
            "Val loss: 0.34848775625228884, Val f1: 0.87615966796875\n",
            "Val loss: 0.34659906332172563, Val f1: 0.8720066547393799\n",
            "Val loss: 0.3458053668340047, Val f1: 0.8694518208503723\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0223926305770874, Val f1: 1.5458316802978516\n",
            "Val loss: 0.6696966489156088, Val f1: 1.0325406789779663\n",
            "Val loss: 0.607823395729065, Val f1: 0.9205359816551208\n",
            "Val loss: 0.5803778171539307, Val f1: 0.8739690780639648\n",
            "Val loss: 0.5600923697153727, Val f1: 0.8501473665237427\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.37723574601113796\n",
            "Train loss: 0.362788329521815\n",
            "Train loss: 0.359708012342453\n",
            "Train loss: 0.3587087068095136\n",
            "Train loss: 0.35909517960889\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3576103709638119, Val f1: 0.9201486110687256\n",
            "Val loss: 0.3485753933588664, Val f1: 0.8895213603973389\n",
            "Val loss: 0.3448392367362976, Val f1: 0.8803650736808777\n",
            "Val loss: 0.3441487443980886, Val f1: 0.874846339225769\n",
            "Val loss: 0.34369271745284397, Val f1: 0.8717561364173889\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0358701944351196, Val f1: 1.5466290712356567\n",
            "Val loss: 0.6787892778714498, Val f1: 1.0311284065246582\n",
            "Val loss: 0.616482949256897, Val f1: 0.9180761575698853\n",
            "Val loss: 0.5885628632136753, Val f1: 0.8716302514076233\n",
            "Val loss: 0.5687186022599539, Val f1: 0.8471161723136902\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.35366172529757023\n",
            "Train loss: 0.3487066491083665\n",
            "Train loss: 0.35024273335933687\n",
            "Train loss: 0.351909865639103\n",
            "Train loss: 0.3511966906842731\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3477942254394293, Val f1: 0.9129161238670349\n",
            "Val loss: 0.33173118938099255, Val f1: 0.8892242312431335\n",
            "Val loss: 0.3311336290836334, Val f1: 0.8797875642776489\n",
            "Val loss: 0.32988499349622585, Val f1: 0.8757610321044922\n",
            "Val loss: 0.3303527931372325, Val f1: 0.8730683922767639\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.064595878124237, Val f1: 1.5307267904281616\n",
            "Val loss: 0.6965430577596029, Val f1: 1.0214223861694336\n",
            "Val loss: 0.6313244581222535, Val f1: 0.9099603891372681\n",
            "Val loss: 0.6032320942197528, Val f1: 0.8640081286430359\n",
            "Val loss: 0.5822590721978081, Val f1: 0.8402203321456909\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.3563249260187149\n",
            "Train loss: 0.345686497110309\n",
            "Train loss: 0.3445944428443909\n",
            "Train loss: 0.3433714175402229\n",
            "Train loss: 0.3427122366570291\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3409327566623688, Val f1: 0.9230143427848816\n",
            "Val loss: 0.33005102656104346, Val f1: 0.8948344588279724\n",
            "Val loss: 0.32604018032550813, Val f1: 0.8852414488792419\n",
            "Val loss: 0.3250958101073308, Val f1: 0.8800808191299438\n",
            "Val loss: 0.3241054912408193, Val f1: 0.8776803016662598\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0711037516593933, Val f1: 1.5348973274230957\n",
            "Val loss: 0.7000874479611715, Val f1: 1.0221995115280151\n",
            "Val loss: 0.6354250550270081, Val f1: 0.9098092317581177\n",
            "Val loss: 0.6077902572495597, Val f1: 0.8637031316757202\n",
            "Val loss: 0.5870190064112345, Val f1: 0.839917778968811\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.3486299328505993\n",
            "Train loss: 0.3392053363901196\n",
            "Train loss: 0.33674256324768065\n",
            "Train loss: 0.3379008102772841\n",
            "Train loss: 0.33808351556460065\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.33760215900838375, Val f1: 0.9263560771942139\n",
            "Val loss: 0.3316478304790728, Val f1: 0.8943480849266052\n",
            "Val loss: 0.3261071342229843, Val f1: 0.8886345624923706\n",
            "Val loss: 0.3249474078861635, Val f1: 0.8835186958312988\n",
            "Val loss: 0.32434469780751635, Val f1: 0.8809202313423157\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0715906620025635, Val f1: 1.5414199829101562\n",
            "Val loss: 0.7014146248499552, Val f1: 1.0287750959396362\n",
            "Val loss: 0.637331485748291, Val f1: 0.9151193499565125\n",
            "Val loss: 0.6097114086151123, Val f1: 0.8687031269073486\n",
            "Val loss: 0.5896910031636556, Val f1: 0.844515860080719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH_Wd_gmDFkA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "ebebcd99-ad91-4fec-908b-c1fb9eff9e95"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "plt.title('Train')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Losses')\n",
        "plt.grid()\n",
        "ax.plot(losses)\n",
        "ax.plot(losses_eval)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hltcIfL6D6IZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c045c5af-fc0b-4e27-87b1-c9999abda482"
      },
      "source": [
        "losses[-1]"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33413625305349176"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIGEPlj-D6IZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f17fbe-efa8-45e0-9f06-0a887ba2f53f"
      },
      "source": [
        "losses_eval[-1]"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.53072190284729"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W-m46TGD6Ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aedf08f-dfb3-4af5-cf16-f34ce86ed3b5"
      },
      "source": [
        "f1s[-1]"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8709, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtm6yPFjD6Ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b6c2d8-c5e0-4f9e-d3bb-c47dae03bee7"
      },
      "source": [
        "f1s_eval[-1]"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7601, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1usm4Js87jp"
      },
      "source": [
        "val_sentences['predicted']  = predict_word_symbol(model, val_iterator)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPJMf7x_87jq",
        "outputId": "788f05eb-2069-477e-e990-5e2405886864"
      },
      "source": [
        "# TP\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['здоровски погуляли по магазинам с USER пингвины',\n",
              " 'прикольно когда к тебе в бар приходит девушка выпить кофе на мин 1 15 а в итоге зависает у тебя в баре на хороших часа полтора',\n",
              " 'USER не умирай еще тебе предстоит вечер',\n",
              " 'rt USER юмор красота требует жертв придумай смешную подпись к картинке URL',\n",
              " 'давайте уж побыстрее 12 декабря объявляйте выходным днем и переносите на пятницу 13 буду строить планы',\n",
              " 'USER холодно очень и паскаль делает мне страдай d',\n",
              " 'время сна я утомилась дико завтра сложный день 6 5 часов мне на восстановление сил всем доброй ночи',\n",
              " 'ник вернется в сериал полиция гавайев счастьерадость ну правильно а то закончили на самом интересном',\n",
              " 'USER смешно ему и плюс мне масса нужна в спорте немного я как бить буду если сил нету крч диета для идиотов',\n",
              " 'USER иногда хочется тебя ретвитить но функция недоступна']"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmiHjCy787jr",
        "outputId": "cbb97b4a-876d-4e73-f592-0d7a72dfb087"
      },
      "source": [
        "# FN\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER ну у меня времени много могу поиграть в шерлока но лучше скажи',\n",
              " 'я только недавно хотела зимы теперь хочу лета а у лета я буду просить зимы',\n",
              " 'поздравили любимого хореографа она была в восторге обожаем ее',\n",
              " 'USER ну я пока еще с тюленем не танцевала',\n",
              " 'USER USER другим не стыдно другим все равно',\n",
              " 'блин аж н як забути цього не можу да не хочу боже так мило',\n",
              " 'USER сломался он не знаю от чего но когда на него звонишь он говорит тебе бай бай так я теперь чика со старым телефоном',\n",
              " 'i am training my english большой привет москве и санкт петербургу в and nyc',\n",
              " 'USER на словах ты лев толстой а на деле хуй простой кароче нахуй послан окончательно терпила все видели что ты зассал',\n",
              " '9летняя девочка называет меня киса и чешет бороду приходится мурчать']"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLSmhXRi87jr",
        "outputId": "35b86fc7-f1aa-45a2-f4f1-dda62f9e77b6"
      },
      "source": [
        "# FP\n",
        "val_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER это дааа куда хочешь поступить',\n",
              " 'USER ну я всегда говорил что я тп',\n",
              " 'кто нибудь зайдите вк USER USER USER',\n",
              " 'rt USER епт так ты баба нет кто тебе сказал такую глупость у меня есть пипирка URL',\n",
              " 'что то много людей сейчас болеют выздоравливайте зайки',\n",
              " 'USER каждый одинок по своему',\n",
              " 'не знаю на сколько сегодня в универ а все говорят разное время',\n",
              " 'помню как под эту песню я пытался мутить с ксюшей прэй фо ми блять',\n",
              " 'USER нас курсач еще только ждет на антиплагиате проверять курсовые будут',\n",
              " 'во мне две половины разрывается между желанием рожать детей и желанием жрать лсд и нюхать кокс с 37сантиметрового члена']"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Q0Z622YDb_"
      },
      "source": [
        "TP: здесь результаты не особо изменились, все еще выводятся высказывания с положительным контекстом\n",
        "\n",
        "FN: здесь результаты не особо изменились, скорее просто скоратились\n",
        "\n",
        "FP: здесь появились высказывания, выражающие сомнения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkBnNigJAzzb"
      },
      "source": [
        "#Эксперимент"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-1UeStc_DvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db43430-ed0f-459d-a4ce-60ba6e42acea"
      },
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(text.split())\n",
        "print('всего уникальных токенов:', len(vocab))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 305559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5YUW50jByph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb8253b-5f2b-4abd-b971-bb19611f4115"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, втретившихся больше 2 раз: 37585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTz2XTbdCIrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5faea3b-62bb-43bf-98a0-d632e7c5438c"
      },
      "source": [
        "symbol_vocab = Counter()\n",
        "\n",
        "for text in tweets_data['text']:\n",
        "    symbol_vocab.update(list(text))\n",
        "print('всего уникальных символов:', len(symbol_vocab))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miEtsa-DCIrt"
      },
      "source": [
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in symbol_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DYJ8pzSByxO"
      },
      "source": [
        "class ExpDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = torch.Tensor(dataset['tone'].values)\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.dataset[index].split() # токенизируем\n",
        "        word_ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        symbols = list(self.dataset[index])\n",
        "        symb_ids = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return word_ids, symb_ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      word_ids, symb_ids, y = list(zip(*batch))\n",
        "      padded_words = pad_sequence(word_ids, batch_first=True).to(self.device)\n",
        "      padded_symbs = pad_sequence(symb_ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_words, padded_symbs, y"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prfsu8zWCt_y"
      },
      "source": [
        "train_dataset = ExpDataset(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi9vVKNoCt_z"
      },
      "source": [
        "val_dataset = ExpDataset(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljNnIooAC9oh"
      },
      "source": [
        "model = WordSymbCNN(len(symbol2id), 2, len(word2id))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AArzUHlnC9oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a21a429-5308-40f1-84b8-b884a3b008fd"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7708606459200382\n",
            "Train loss: 0.7268111741904056\n",
            "Train loss: 0.7081413316726685\n",
            "Train loss: 0.6972635655260798\n",
            "Train loss: 0.688353643530891\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7004862502217293, Val f1: 0.6857378482818604\n",
            "Val loss: 0.6794380383058027, Val f1: 0.665271520614624\n",
            "Val loss: 0.6727498316764832, Val f1: 0.6578764915466309\n",
            "Val loss: 0.6690991787768122, Val f1: 0.6550553441047668\n",
            "Val loss: 0.6673052708307902, Val f1: 0.6526967883110046\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3089630603790283, Val f1: 1.330315113067627\n",
            "Val loss: 0.8746776580810547, Val f1: 0.8825599551200867\n",
            "Val loss: 0.7895276308059692, Val f1: 0.7848194241523743\n",
            "Val loss: 0.7519354053906032, Val f1: 0.7477174401283264\n",
            "Val loss: 0.7301345202657912, Val f1: 0.725041389465332\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6782073266804218\n",
            "Train loss: 0.6499359481262438\n",
            "Train loss: 0.6342548167705536\n",
            "Train loss: 0.6222556918414671\n",
            "Train loss: 0.6116975297530493\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6060617119073868, Val f1: 0.8335288763046265\n",
            "Val loss: 0.5870401353547068, Val f1: 0.8070810437202454\n",
            "Val loss: 0.5819795358181, Val f1: 0.7972180843353271\n",
            "Val loss: 0.5788678491293494, Val f1: 0.7939087748527527\n",
            "Val loss: 0.5775739231279918, Val f1: 0.7920684218406677\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1293835639953613, Val f1: 1.585663080215454\n",
            "Val loss: 0.760735829671224, Val f1: 1.0492539405822754\n",
            "Val loss: 0.6843509674072266, Val f1: 0.9389759302139282\n",
            "Val loss: 0.6500048381941659, Val f1: 0.8991776704788208\n",
            "Val loss: 0.6313114364941915, Val f1: 0.8738747239112854\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5526012647897005\n",
            "Train loss: 0.5200841896461718\n",
            "Train loss: 0.4962722706794739\n",
            "Train loss: 0.4726916084538645\n",
            "Train loss: 0.4515131261377108\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38399071618914604, Val f1: 0.933358371257782\n",
            "Val loss: 0.37162521391203907, Val f1: 0.9052873849868774\n",
            "Val loss: 0.3670995634794235, Val f1: 0.8976590633392334\n",
            "Val loss: 0.3655776915265553, Val f1: 0.8931120038032532\n",
            "Val loss: 0.36433036164158866, Val f1: 0.8911487460136414\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7171335816383362, Val f1: 1.7729215621948242\n",
            "Val loss: 0.48875175913174945, Val f1: 1.1695884466171265\n",
            "Val loss: 0.4338713943958282, Val f1: 1.0546993017196655\n",
            "Val loss: 0.4110197935785566, Val f1: 1.0068531036376953\n",
            "Val loss: 0.39820432662963867, Val f1: 0.9799107313156128\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.35930442065000534\n",
            "Train loss: 0.3309171705534964\n",
            "Train loss: 0.3130878496170044\n",
            "Train loss: 0.29915099913504584\n",
            "Train loss: 0.285759204909915\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2398839369416237, Val f1: 0.9903422594070435\n",
            "Val loss: 0.2353709927110961, Val f1: 0.9566402435302734\n",
            "Val loss: 0.23188546568155288, Val f1: 0.948456346988678\n",
            "Val loss: 0.2296196022140446, Val f1: 0.9442811012268066\n",
            "Val loss: 0.22800242865369433, Val f1: 0.942140519618988\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4528326392173767, Val f1: 1.860456109046936\n",
            "Val loss: 0.30840503176053363, Val f1: 1.2366517782211304\n",
            "Val loss: 0.27175856232643125, Val f1: 1.1162919998168945\n",
            "Val loss: 0.2559965231588909, Val f1: 1.0665559768676758\n",
            "Val loss: 0.24760689503616756, Val f1: 1.038405179977417\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.22748676873743534\n",
            "Train loss: 0.20817197904442297\n",
            "Train loss: 0.19608200907707216\n",
            "Train loss: 0.18650686896559018\n",
            "Train loss: 0.17860491289978936\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.19494801200926304, Val f1: 1.0045533180236816\n",
            "Val loss: 0.1860270933671431, Val f1: 0.9754930138587952\n",
            "Val loss: 0.18374575555324554, Val f1: 0.9667607545852661\n",
            "Val loss: 0.18192760246013528, Val f1: 0.9617701172828674\n",
            "Val loss: 0.18003663349719273, Val f1: 0.9597217440605164\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3511379659175873, Val f1: 1.9066426753997803\n",
            "Val loss: 0.24001417557398477, Val f1: 1.2650970220565796\n",
            "Val loss: 0.21147775650024414, Val f1: 1.1413086652755737\n",
            "Val loss: 0.19923265704086848, Val f1: 1.087944746017456\n",
            "Val loss: 0.19295184479819405, Val f1: 1.0572316646575928\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.1478639431297779\n",
            "Train loss: 0.1359703829794219\n",
            "Train loss: 0.12863886401057242\n",
            "Train loss: 0.12154023315924317\n",
            "Train loss: 0.11733317020393554\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.16571347415447235, Val f1: 1.0128954648971558\n",
            "Val loss: 0.16181519988811377, Val f1: 0.9815465807914734\n",
            "Val loss: 0.16164476543664932, Val f1: 0.97089684009552\n",
            "Val loss: 0.16015748813081143, Val f1: 0.9663946628570557\n",
            "Val loss: 0.15953026711940765, Val f1: 0.9633371233940125\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.30779194831848145, Val f1: 1.9157185554504395\n",
            "Val loss: 0.21231403946876526, Val f1: 1.2707862854003906\n",
            "Val loss: 0.18666886687278747, Val f1: 1.1456718444824219\n",
            "Val loss: 0.17646905779838562, Val f1: 1.0915837287902832\n",
            "Val loss: 0.17046905722883013, Val f1: 1.061057686805725\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.09857545886188745\n",
            "Train loss: 0.09280396365758145\n",
            "Train loss: 0.08802887141704559\n",
            "Train loss: 0.08450421301731423\n",
            "Train loss: 0.0814470811968758\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.09799508843570948, Val f1: 1.0365852117538452\n",
            "Val loss: 0.09707498527837521, Val f1: 1.0042046308517456\n",
            "Val loss: 0.09688536554574967, Val f1: 0.9940153360366821\n",
            "Val loss: 0.09601991990608956, Val f1: 0.9890822172164917\n",
            "Val loss: 0.09634844320161003, Val f1: 0.9857864379882812\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.19096894562244415, Val f1: 1.9511182308197021\n",
            "Val loss: 0.13140196104844412, Val f1: 1.2958120107650757\n",
            "Val loss: 0.11433000266551971, Val f1: 1.1676928997039795\n",
            "Val loss: 0.10812847529138837, Val f1: 1.1132060289382935\n",
            "Val loss: 0.10413679149415758, Val f1: 1.0826300382614136\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.0688302656635642\n",
            "Train loss: 0.0652182029955315\n",
            "Train loss: 0.06332485526800155\n",
            "Train loss: 0.06097483646069\n",
            "Train loss: 0.05886582267426309\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.05960276466794312, Val f1: 1.0472276210784912\n",
            "Val loss: 0.05914354121143168, Val f1: 1.015084147453308\n",
            "Val loss: 0.05827039010822773, Val f1: 1.0052152872085571\n",
            "Val loss: 0.05737452153394471, Val f1: 1.0005974769592285\n",
            "Val loss: 0.05699909709039189, Val f1: 0.9975122809410095\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.11865502595901489, Val f1: 1.9706385135650635\n",
            "Val loss: 0.08020440489053726, Val f1: 1.3134348392486572\n",
            "Val loss: 0.06912277042865753, Val f1: 1.1835936307907104\n",
            "Val loss: 0.06536120708499636, Val f1: 1.1272876262664795\n",
            "Val loss: 0.06259747222065926, Val f1: 1.0956469774246216\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.04845789656974375\n",
            "Train loss: 0.04642368480563164\n",
            "Train loss: 0.046144772730767725\n",
            "Train loss: 0.046587899688686894\n",
            "Train loss: 0.046223959663794154\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.060616884380578995, Val f1: 1.0466433763504028\n",
            "Val loss: 0.05953982427264705, Val f1: 1.014735460281372\n",
            "Val loss: 0.05871634043753147, Val f1: 1.0044137239456177\n",
            "Val loss: 0.058966601740068465, Val f1: 0.9992004632949829\n",
            "Val loss: 0.058950623674761685, Val f1: 0.996207594871521\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.1211487352848053, Val f1: 1.9734992980957031\n",
            "Val loss: 0.08328086137771606, Val f1: 1.3122389316558838\n",
            "Val loss: 0.07164400219917297, Val f1: 1.18210768699646\n",
            "Val loss: 0.06797395699790545, Val f1: 1.1257050037384033\n",
            "Val loss: 0.06450410311420758, Val f1: 1.0945333242416382\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.04504051385447383\n",
            "Train loss: 0.040933488444848495\n",
            "Train loss: 0.04009084317833185\n",
            "Train loss: 0.03982349287774136\n",
            "Train loss: 0.03909751792837467\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.05411831149831414, Val f1: 1.047286868095398\n",
            "Val loss: 0.05228831957687031, Val f1: 1.0155439376831055\n",
            "Val loss: 0.05259653136134148, Val f1: 1.005660891532898\n",
            "Val loss: 0.05103700146523874, Val f1: 1.0012298822402954\n",
            "Val loss: 0.05169247192818494, Val f1: 0.9978983998298645\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.1077556349337101, Val f1: 1.9744305610656738\n",
            "Val loss: 0.07471790537238121, Val f1: 1.3140782117843628\n",
            "Val loss: 0.06383021101355553, Val f1: 1.1843584775924683\n",
            "Val loss: 0.06074016700897898, Val f1: 1.1274436712265015\n",
            "Val loss: 0.057168039596743055, Val f1: 1.0963026285171509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPjECZpHCtvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "295f4e6b-c3b3-4b99-8f3d-bc32262627a2"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "plt.title('Train')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Losses')\n",
        "plt.grid()\n",
        "ax.plot(losses)\n",
        "ax.plot(losses_eval)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SggFgQfIDOiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34bbb6f-49a0-4376-c802-a4facb62ca5c"
      },
      "source": [
        "losses[-1]"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.038379818222231486"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5woq7ZDUqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82ff538-781e-4fc8-8161-b86fea9ca4b1"
      },
      "source": [
        "losses_eval[-1]"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05145123563706875"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhhiuZ0qDZHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5dd4b3-26d4-4b26-f1ed-be402d832166"
      },
      "source": [
        "f1s[-1]"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9861, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_adey47uDbmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ee0acb-96ba-4ec7-be24-0b3eb575a96d"
      },
      "source": [
        "f1s_eval[-1]"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9867, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9X6lr5e89YK"
      },
      "source": [
        "val_sentences['predicted']  = predict_word_symbol(model, val_iterator)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkiiBtrK89YK",
        "outputId": "f2d3bb69-5b18-451f-b02e-69b9810778be"
      },
      "source": [
        "# TP\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['здоровски погуляли по магазинам с USER пингвины',\n",
              " 'прикольно когда к тебе в бар приходит девушка выпить кофе на мин 1 15 а в итоге зависает у тебя в баре на хороших часа полтора',\n",
              " 'USER не умирай еще тебе предстоит вечер',\n",
              " 'rt USER юмор красота требует жертв придумай смешную подпись к картинке URL',\n",
              " 'давайте уж побыстрее 12 декабря объявляйте выходным днем и переносите на пятницу 13 буду строить планы',\n",
              " 'USER холодно очень и паскаль делает мне страдай d',\n",
              " 'время сна я утомилась дико завтра сложный день 6 5 часов мне на восстановление сил всем доброй ночи',\n",
              " 'ник вернется в сериал полиция гавайев счастьерадость ну правильно а то закончили на самом интересном',\n",
              " 'USER ну у меня времени много могу поиграть в шерлока но лучше скажи',\n",
              " 'USER смешно ему и плюс мне масса нужна в спорте немного я как бить буду если сил нету крч диета для идиотов']"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3JHt8cA89YL",
        "outputId": "46150fa9-1142-4c40-c8b9-53a286fea902"
      },
      "source": [
        "# FN\n",
        "val_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER я тебя хоть трахну если попросишь d',\n",
              " 'сегодня катерины в честь этого сделала себе роллов вообще в последнее время ем столько японской еды что скоро стану девочкой из анимэ',\n",
              " 'скачала приложение simi и вот что он мне выдал пидорас такой d URL',\n",
              " 'everything is made in china выступят в петербургском клубе da da URL']"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skORHvL189YL",
        "outputId": "3e6ac952-8b3b-4b9c-85f1-ae590060a071"
      },
      "source": [
        "# FP\n",
        "val_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)]['clean_text'].to_list()[:10]"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['школьная дискотека дает о себе знать синяк на коленке несгинающаяся шея и дождик в волосах',\n",
              " 'USER очень жестка я сегодня хотел пойти но мне мама не разрешила хныык',\n",
              " 'USER с както печально а что рисовать любишь',\n",
              " 'USER USER USER USER USER USER USER и тебя с начинающим 3',\n",
              " 'rt USER после проведения плазмолифтинга лица процесс омоложения с каждым месяцем усиливается URL',\n",
              " 'USER та я тож если чесн как дела',\n",
              " 'USER обычный белый белый с розовым и белый в шоколаде с',\n",
              " 'USER что случилось не плааач дорогая',\n",
              " 'USER и хорошо что все в порядке с',\n",
              " 'USER USER USER USER это лишь ваше субъективное мнение']"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXSUnoalSEHd"
      },
      "source": [
        "TP: результаты почти не поменялись\n",
        "\n",
        "FN: результаты сильно поменялись, видимо из-за 10-ти эпох\n",
        "\n",
        "FP: результаты почти не поменялись"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuhPDVgyLMEv"
      },
      "source": [
        ""
      ],
      "execution_count": 167,
      "outputs": []
    }
  ]
}